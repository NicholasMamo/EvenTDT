

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta content="The Natural Language Processing (NLP) library" name="description" />
<meta content="Python, TDT, NLP" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. Natural Language Processing (NLP) &mdash; EvenTDT 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/eventdt.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Wikinterface" href="wikinterface.html" />
    <link rel="prev" title="2. Vector Space Model (VSM)" href="vsm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> EvenTDT
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="config.html">0. Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">1. Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="vsm.html">2. Vector Space Model (VSM)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. Natural Language Processing (NLP)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-nlp.document">Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nlp.tokenizer">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nlp.weighting">Term-Weighting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-term-weighting-schemes">Common Term-Weighting Schemes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-nlp.weighting.tf">Term Frequency (TF)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nlp.weighting.tfidf">Term Frequency-Inverse Document Frequency (TF-IDF)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-nlp.weighting.local_schemes">Local Term-Weighting Schemes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-nlp.weighting.local_schemes.boolean">Boolean</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Term Frequency (TF)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-nlp.weighting.global_schemes">Global Term-Weighting Schemes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-nlp.weighting.global_schemes.filler">Filler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nlp.weighting.global_schemes.idf">Inverse Document Frequency (IDF)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-nlp.cleaners">Cleaners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-nlp.cleaners.tweet_cleaner">Tweet Cleaner</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="wikinterface.html">4. Wikinterface</a></li>
<li class="toctree-l1"><a class="reference internal" href="apd.html">5. Automatic Participant Detection (APD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tdt.html">6. Topic Detection and Tracking (TDT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="summarization.html">7. Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="twitter.html">8. Twitter</a></li>
<li class="toctree-l1"><a class="reference internal" href="consumers.html">9. Consumers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml.html">10. Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ate.html">11. Automatic Term Extraction (ATE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="other.html">12. Other</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">EvenTDT</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>3. Natural Language Processing (NLP)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/nlp.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
    <div class="section" id="natural-language-processing-nlp">
<h1>3. Natural Language Processing (NLP)<a class="headerlink" href="#natural-language-processing-nlp" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-nlp"><span id="nlp"></span></span><p>Text mining tasks generally depend a lot on the document representation.
EvenTDT provides functionality to make it easier to work with documents:</p>
<ol class="arabic simple">
<li><p>The <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> class, based on the <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> class, but with text-specific functionality;</p></li>
<li><p>The <a class="reference internal" href="#nlp.tokenizer.Tokenizer" title="nlp.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a> class to split a piece of text into tokens; and</p></li>
<li><p>The <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> abstract class, as well as different term-weighting schemes, to assign weight to terms.</p></li>
</ol>
<div class="section" id="module-nlp.document">
<span id="documents"></span><h2>Documents<a class="headerlink" href="#module-nlp.document" title="Permalink to this headline">¶</a></h2>
<p>Many NLP tasks represent text as documents.
The <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> class helps you do just that.
The <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> class builds on the <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> class, which means it represents text in the <a class="reference internal" href="vsm.html#vsm.vector.VectorSpace" title="vsm.vector.VectorSpace"><code class="xref py py-class docutils literal notranslate"><span class="pre">VectorSpace</span></code></a>.
The big change between the <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> and the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> is that the latter stores the original text alongside the VSM dimensions.</p>
<p>You can create documents by instantiating the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> class.
However, more generally you would follow these two steps:</p>
<ol class="arabic simple">
<li><p>Convert the text into tokens using a <a class="reference internal" href="#nlp.tokenizer.Tokenizer" title="nlp.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a>.</p></li>
<li><p>Weight the tokens using a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a>.
This automatically transforms tokens into <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> dimensions and create a <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> for you.</p></li>
</ol>
<dl class="class">
<dt id="nlp.document.Document">
<em class="property">class </em><code class="sig-name descname">Document</code><span class="sig-paren">(</span><em class="sig-param">text=''</em>, <em class="sig-param">dimensions=None</em>, <em class="sig-param">scheme=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> class is based on the <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> class.
The only big change is the <code class="docutils literal notranslate"><span class="pre">text</span></code> instance variable.
Since a <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> is represented as a <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a>, the <code class="docutils literal notranslate"><span class="pre">text</span></code> records the original text that generated the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><p><a class="reference internal" href="wikinterface.html#module-wikinterface.text" title="wikinterface.text"><strong>text</strong></a> (<em>str</em>) – The document’s original text.</p>
</dd>
</dl>
<dl class="method">
<dt id="nlp.document.Document.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">text=''</em>, <em class="sig-param">dimensions=None</em>, <em class="sig-param">scheme=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the document with the text and, optionally, the underlying vector’s dimensions.</p>
<p>If you only have tokens generated by a <a class="reference internal" href="#nlp.tokenizer.Tokenizer" title="nlp.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a>, you can pass them on as dimensions.
In this case, the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> creates dimensions using the <a class="reference internal" href="#nlp.weighting.tf.TF" title="nlp.weighting.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> term-weighting scheme.
If you want to use a different term-weighting scheme, pass it on using the <code class="docutils literal notranslate"><span class="pre">scheme</span></code> parameter.</p>
<p>Any other arguments or keyword arguments are passed on to the <a class="reference internal" href="vsm.html#vsm.vector.Vector" title="vsm.vector.Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> constructor.
You can use the keyword arguments to pass on any optional attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – The document’s text.</p></li>
<li><p><strong>dimensions</strong> (<em>list</em><em> or </em><em>dict</em>) – The initial dimensions of the document.
If a list is provided, it is assumed that they are tokens.
The dimensions are then created from this list using the given scheme.</p></li>
<li><p><strong>scheme</strong> (None or <code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeighting</span></code>) – The term-weighting scheme that is used to convert the tokens into dimensions.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> is given, the <code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code> term-weighting scheme is used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.document.Document.__str__">
<code class="sig-name descname">__str__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.__str__" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the string representation of the document.
This function returns the document’s text.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The text of the document.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.document.Document.copy">
<code class="sig-name descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a copy of the document.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A copy of the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.document.Document.to_array">
<code class="sig-name descname">to_array</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.to_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Export the document as an associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The document as an associative array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.document.Document.from_array">
<em class="property">static </em><code class="sig-name descname">from_array</code><span class="sig-paren">(</span><em class="sig-param">array</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.from_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an instance of the document from the given associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>array</strong> (<em>dict</em>) – The associative array with the attributes to create the document.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new instance of an object with the same attributes stored in the object.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.document.Document.concatenate">
<em class="property">static </em><code class="sig-name descname">concatenate</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">tokenizer</em>, <em class="sig-param">scheme=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.document.Document.concatenate" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenate all of the documents that are provided as arguments.
To concatenate the documents, the function:</p>
<ol class="arabic simple">
<li><p>Concatenates the text of all documents, in the same order as they are given.</p></li>
<li><p>Tokenizes the concatenated text using the given tokenizer.</p></li>
<li><p>Creates a document with the tokens from the concatenated document.</p></li>
</ol>
<p>Any additional keyword arguments, such as attributes, are passed on to the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scheme</strong> (<a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a>) – The term-weighting scheme to use to create the concatenated document.</p></li>
<li><p><strong>tokenizer</strong> (<a class="reference internal" href="#nlp.tokenizer.Tokenizer" title="nlp.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a>) – The tokenizer to use to construct the concatenated document.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new document representing the concatenated documents.
The document is not normalized.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp.tokenizer">
<span id="tokenization"></span><h2>Tokenization<a class="headerlink" href="#module-nlp.tokenizer" title="Permalink to this headline">¶</a></h2>
<p>The tokenizer is the first step to create a <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>.
The tokenizer takes in plain text and splits it into tokens, often words, that make up the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>’s dimensions.
You would usually follow up tokenization with a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> to assign a weight to the tokens.</p>
<p>The tokenizer has many capabilities, and these are all enabled or disabled in the constructor.
In this way, all documents are tokenized in the same way.
After creating a <a class="reference internal" href="#nlp.tokenizer.Tokenizer" title="nlp.tokenizer.Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tokenizer</span></code></a>, use the <a class="reference internal" href="#nlp.tokenizer.Tokenizer.tokenize" title="nlp.tokenizer.Tokenizer.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a> function to extract the tokens:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">stem</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">split_hashtags</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;Hello world!&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Stemming is based on, and requires, NLTK.
You can install NLTK as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install nltk
</pre></div>
</div>
<p>After that, you may need to use NLTK to download additional resources.
If you receive an error, pay attention and follow the on-screen suggestions to download and install any missing corpora.</p>
</div>
<dl class="class">
<dt id="nlp.tokenizer.Tokenizer">
<em class="property">class </em><code class="sig-name descname">Tokenizer</code><span class="sig-paren">(</span><em class="sig-param">remove_mentions=True</em>, <em class="sig-param">remove_hashtags=False</em>, <em class="sig-param">split_hashtags=True</em>, <em class="sig-param">remove_numbers=True</em>, <em class="sig-param">remove_urls=True</em>, <em class="sig-param">remove_alt_codes=True</em>, <em class="sig-param">normalize_words=False</em>, <em class="sig-param">character_normalization_count=3</em>, <em class="sig-param">case_fold=True</em>, <em class="sig-param">remove_punctuation=True</em>, <em class="sig-param">remove_unicode_entities=False</em>, <em class="sig-param">min_length=3</em>, <em class="sig-param">stopwords=None</em>, <em class="sig-param">stem=True</em>, <em class="sig-param">normalize_special_characters=True</em>, <em class="sig-param">pos=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.tokenizer.Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>The tokenizer takes in plain text and converts it into tokens.
The way it extracts tokens depends on how you create the tokenizer.
All of the settings are passed on to the constructor so that the tokenizer always extracts tokens in the same way.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Not all settings are required.
In fact, it is very uncommon that you would need to specify all options.</p>
</div>
<p>Apart from the settings you provide, the tokenizer also creates a stemmer and stores it as an instance variable.
The tokenizer re-uses this stemmer to save time.
In addition, it caches stems so that if it encounters a word multiple times, it re-uses the old stem.</p>
<p>After creating the tokenizer, call the <a class="reference internal" href="#nlp.tokenizer.Tokenizer.tokenize" title="nlp.tokenizer.Tokenizer.tokenize"><code class="xref py py-func docutils literal notranslate"><span class="pre">tokenize()</span></code></a> to split text into tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stemmer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.stem.porter.PorterStemmer</span></code>) – The Porter Stemmer used by the class.</p></li>
<li><p><strong>stem_cache</strong> (<em>dict</em>) – A mapping from tokens to their stems.
The keys are the original tokens and the values are their stems.
This is used to speed up tokenization of large corpora.</p></li>
<li><p><strong>remove_mentions</strong> (<em>bool</em>) – A boolean indicating whether mentions (such as <cite>&#64;NicholasMamo</cite>) should be removed.</p></li>
<li><p><strong>remove_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags (<cite>#EvenTDT</cite>) should be removed.</p></li>
<li><p><strong>split_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags should be normalized.
This converts hashtags of the type #ManchesterUnited to <cite>Manchester United</cite>, based on camel-case.</p></li>
<li><p><strong>remove_numbers</strong> (<em>bool</em>) – A boolean indicating whether numbers should be removed.</p></li>
<li><p><strong>remove_urls</strong> (<em>bool</em>) – A boolean indicating whether URLs should be removed.</p></li>
<li><p><strong>remove_alt_codes</strong> (<em>bool</em>) – A boolean indicating whether ALT-codes should be removed.</p></li>
<li><p><strong>normalize_words</strong> (<em>bool</em>) – A boolean indicating whether words should be normalized.
This removes repeated characters.
For example, <cite>goooaaaal</cite> becomes <cite>goal</cite>.
The number of repeated characters that are removed is controlled using the <cite>character_normalization_count</cite> parameter.</p></li>
<li><p><strong>character_normalization_count</strong> (<em>int</em>) – The number of times a character is repeated before they are reduced to one.</p></li>
<li><p><strong>case_fold</strong> (<em>bool</em>) – A boolean indicating whether the tokens should be case-folded to lowercase.</p></li>
<li><p><strong>remove_punctuation</strong> (<em>bool</em>) – A boolean indicating whether punctuation should be removed.</p></li>
<li><p><strong>remove_unicode_entities</strong> (<em>bool</em>) – A boolean indicating whether unicode entities should be removed.</p></li>
<li><p><strong>min_length</strong> (<em>int</em>) – The minimum length of tokens that should be retained.</p></li>
<li><p><strong>stopwords</strong> (<em>list</em>) – A list of stopwords.
Stopwords are common words that are removed from token lists because they are not very expressive.
Normally, stopwords are provided as a list, and then converted into a dict, where the stopwords are the keys.
This approach is adopted since Python uses hashing to check whether a key is in a dict.
However, they may also be provided as a dict directly.</p></li>
<li><p><strong>stem</strong> (<em>bool</em>) – A boolean indicating whether the tokens should be stemmed.</p></li>
<li><p><strong>normalize_special_characters</strong> (<em>bool</em>) – A boolean indicating whether accents should be removed and replaced with simple unicode characters.</p></li>
<li><p><strong>url_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify URLs in the text.</p></li>
<li><p><strong>alt_code_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify and remove alt-codes from the text.</p></li>
<li><p><strong>mention_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify mentions in the text.</p></li>
<li><p><strong>hashtag_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify hashtags in the text.</p></li>
<li><p><strong>word_normalization_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify words with repeated characters.</p></li>
<li><p><strong>number_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify numbers in the text.</p></li>
<li><p><strong>tokenize_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to split the text into tokens.</p></li>
<li><p><strong>camel_case_pattern</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">re.Pattern</span></code>) – The pattern used to identify camel-case letters.</p></li>
<li><p><a class="reference internal" href="#nlp.tokenizer.Tokenizer.pos" title="nlp.tokenizer.Tokenizer.pos"><strong>pos</strong></a> (<em>None</em><em> or </em><em>list of str</em>) – <p>The parts of speech tags to keep.
If <cite>None</cite> is given, all tokens are retained.
For a list of possible tags and their meanings, run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">upenn_tagset</span><span class="p">()</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nlp.tokenizer.Tokenizer.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">remove_mentions=True</em>, <em class="sig-param">remove_hashtags=False</em>, <em class="sig-param">split_hashtags=True</em>, <em class="sig-param">remove_numbers=True</em>, <em class="sig-param">remove_urls=True</em>, <em class="sig-param">remove_alt_codes=True</em>, <em class="sig-param">normalize_words=False</em>, <em class="sig-param">character_normalization_count=3</em>, <em class="sig-param">case_fold=True</em>, <em class="sig-param">remove_punctuation=True</em>, <em class="sig-param">remove_unicode_entities=False</em>, <em class="sig-param">min_length=3</em>, <em class="sig-param">stopwords=None</em>, <em class="sig-param">stem=True</em>, <em class="sig-param">normalize_special_characters=True</em>, <em class="sig-param">pos=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.tokenizer.Tokenizer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>remove_mentions</strong> (<em>bool</em>) – A boolean indicating whether mentions (such as <cite>&#64;NicholasMamo</cite>) should be removed.</p></li>
<li><p><strong>remove_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags (<cite>#EvenTDT</cite>) should be removed.</p></li>
<li><p><strong>split_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags should be normalized.
This converts hashtags of the type #ManchesterUnited to <cite>Manchester United</cite>, based on camel-case.</p></li>
<li><p><strong>remove_numbers</strong> (<em>bool</em>) – A boolean indicating whether numbers should be removed.</p></li>
<li><p><strong>remove_urls</strong> (<em>bool</em>) – A boolean indicating whether URLs should be removed.</p></li>
<li><p><strong>remove_alt_codes</strong> (<em>bool</em>) – A boolean indicating whether ALT-codes should be removed.</p></li>
<li><p><strong>normalize_words</strong> (<em>bool</em>) – A boolean indicating whether words should be normalized.
This removes repeated characters.
For example, <cite>goooaaaal</cite> becomes <cite>goal</cite>.
The number of repeated characters that are removed is controlled using the <cite>character_normalization_count</cite> parameter.</p></li>
<li><p><strong>character_normalization_count</strong> (<em>int</em>) – The number of times a character is repeated before they are reduced to one.</p></li>
<li><p><strong>case_fold</strong> (<em>bool</em>) – A boolean indicating whether the tokens should be case-folded to lowercase.</p></li>
<li><p><strong>remove_punctuation</strong> (<em>bool</em>) – A boolean indicating whether punctuation should be removed.</p></li>
<li><p><strong>remove_unicode_entities</strong> (<em>bool</em>) – A boolean indicating whether unicode entities should be removed.
Note that this also includes emojis.</p></li>
<li><p><strong>min_length</strong> (<em>int</em>) – The minimum length of tokens that should be retained.</p></li>
<li><p><strong>stopwords</strong> (<em>list</em>) – A list of stopwords: common words that are removed from token lists.
This is based on the assumption that they are not very expressive.
Normally, stopwords are provided as a list, and then converted into a dict, where the stopwords are the keys.
This approach is adopted since Python uses hashing to check whether a key is in a dict.
However, they may also be provided as a dict directly.</p></li>
<li><p><strong>stem</strong> (<em>bool</em>) – A boolean indicating whether the tokens should be stemmed.</p></li>
<li><p><strong>normalize_special_characters</strong> (<em>bool</em>) – A boolean indicating whether accents should be removed and replaced with simple unicode characters.</p></li>
<li><p><strong>pos</strong> (<em>None</em><em> or </em><em>list of str</em>) – <p>The parts of speech tags to keep.
If <cite>None</cite> is given, all tokens are retained.
For a list of possible tags and their meanings, run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">upenn_tagset</span><span class="p">()</span>
</pre></div>
</div>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nlp.tokenizer.Tokenizer.pos">
<code class="sig-name descname">pos</code><em class="property"> = None</em><a class="headerlink" href="#nlp.tokenizer.Tokenizer.pos" title="Permalink to this definition">¶</a></dt>
<dd><p>The list of regular expressions to be used.</p>
</dd></dl>

<dl class="method">
<dt id="nlp.tokenizer.Tokenizer.tokenize">
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param">text</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.tokenizer.Tokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize the given text based on the parameters specified in the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The text to tokenize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nlp.tokenizer.Tokenizer.__weakref__">
<code class="sig-name descname">__weakref__</code><a class="headerlink" href="#nlp.tokenizer.Tokenizer.__weakref__" title="Permalink to this definition">¶</a></dt>
<dd><p>list of weak references to the object (if defined)</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp.weighting">
<span id="term-weighting"></span><h2>Term-Weighting<a class="headerlink" href="#module-nlp.weighting" title="Permalink to this headline">¶</a></h2>
<p>Term-weighting schemes are responsible for assigning importance to terms.
Term-weighting schemes are made up of:</p>
<ol class="arabic simple">
<li><p>A local component, which scores terms based on their appearance in a document.</p></li>
<li><p>A global component, which scores terms based on their appearance in a corpus.</p></li>
<li><p>Optionally, a normalization component to make all documents similar.</p></li>
</ol>
<p>This package represents term-weighting schemes in the same way, but without normalization.
So, a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> has two components:</p>
<ol class="arabic simple">
<li><p>A local component (you can read more about them <a class="reference internal" href="#nlp-local"><span class="std std-ref">here</span></a>), and</p></li>
<li><p>A global component (you can read more about them <a class="reference internal" href="#nlp-local"><span class="std std-ref">here</span></a>).</p></li>
</ol>
<p>The <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a>’s <a class="reference internal" href="#nlp.weighting.TermWeightingScheme.create" title="nlp.weighting.TermWeightingScheme.create"><code class="xref py py-func docutils literal notranslate"><span class="pre">create()</span></code></a> uses its local and global term-weighting schemes to create a <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>.
This function combines the score using a simple product.</p>
<p>You can combine any local term-weighting scheme with any other global term-weighting scheme.
All local and global term-weighting schemes inherit the <a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>.
That means all term-weighting schemes must implement the <a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">score()</span></code> to assign a score to terms.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Normally, you don’t have to create your own term-weighting schemes.
EvenTDT includes some of the most common term-weighting schemes to help you get started faster.
You can read more about these readily-available term-weighting schemes <a class="reference internal" href="#nlp-common"><span class="std std-ref">here</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can read more about term-weighting schemes in <a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/0306457388900210">Term-Weighting Approaches in Automatic Text Retrieval by Salton and Buckley (1998)</a>.</p>
</div>
<dl class="class">
<dt id="nlp.weighting.TermWeightingScheme">
<em class="property">class </em><code class="sig-name descname">TermWeightingScheme</code><span class="sig-paren">(</span><em class="sig-param">local_scheme</em>, <em class="sig-param">global_scheme</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.TermWeightingScheme" title="Permalink to this definition">¶</a></dt>
<dd><p>A term-weighting scheme is made up of a local term-weighting scheme and a global term-weighting scheme.
Each one gives a score to a token, and the term-weighting scheme combines the scores into one weight.</p>
<p>This class stores the local and global term-weighting schemes.
You can provide any combination of schemes in the constructor.
After creating a term-weighting scheme, use the <a class="reference internal" href="#nlp.weighting.TermWeightingScheme.create" title="nlp.weighting.TermWeightingScheme.create"><code class="xref py py-func docutils literal notranslate"><span class="pre">create()</span></code></a> function to convert tokens into a document.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_scheme</strong> (<a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>) – The local term-weighting scheme.</p></li>
<li><p><strong>global_scheme</strong> (<a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>) – The global term-weighting scheme.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nlp.weighting.TermWeightingScheme.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">local_scheme</em>, <em class="sig-param">global_scheme</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.TermWeightingScheme.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>A term-weighting scheme is made up of a local component and a global component.
Each term-weighting scheme contributes a score, which when multiplied give the final token weight.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>local_scheme</strong> (<a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>) – The local term-weighting scheme.</p></li>
<li><p><strong>global_scheme</strong> (<a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>) – The global term-weighting scheme.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.TermWeightingScheme.create">
<code class="sig-name descname">create</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.TermWeightingScheme.create" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> from the given tokens.
This function calls the <a class="reference internal" href="#nlp.weighting.SchemeScorer.score" title="nlp.weighting.SchemeScorer.score"><code class="xref py py-func docutils literal notranslate"><span class="pre">score()</span></code></a> function on the local and global term-weighting schemes.
Then, it multiplies them together to get the final token weight.</p>
<p>The function accepts other arguments or keyword arguments and passes them on to the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> constructor.
You can use these arguments to instantiate the <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a> with <code class="docutils literal notranslate"><span class="pre">text</span></code> or <code class="docutils literal notranslate"><span class="pre">attributes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – A list of tokens that can be converted into dimensions.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new document with the given tokens as the dimensions.
The dimensions’ weights depend on the local and global term-weighting scheme.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nlp.weighting.TermWeightingScheme.__weakref__">
<code class="sig-name descname">__weakref__</code><a class="headerlink" href="#nlp.weighting.TermWeightingScheme.__weakref__" title="Permalink to this definition">¶</a></dt>
<dd><p>list of weak references to the object (if defined)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp.weighting.SchemeScorer">
<em class="property">class </em><code class="sig-name descname">SchemeScorer</code><a class="headerlink" href="#nlp.weighting.SchemeScorer" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> combines two <a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a> scores to weight tokens.
This class defines the functionality that all local and global term-weighting schemes must have at least.</p>
<p>Essentially, the <a class="reference internal" href="#nlp.weighting.SchemeScorer.score" title="nlp.weighting.SchemeScorer.score"><code class="xref py py-func docutils literal notranslate"><span class="pre">score()</span></code></a> function needs to be implemented.
This function receives a list of tokens and assigns them a score.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to distinguish between <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> and <a class="reference internal" href="#nlp.weighting.SchemeScorer" title="nlp.weighting.SchemeScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchemeScorer</span></code></a>.
The former is a complete term-weighting scheme that takes local and global scheme scorers and combines their token scores.
The latter is the component that actually scores tokens.</p>
</div>
<dl class="method">
<dt id="nlp.weighting.SchemeScorer.score">
<em class="property">abstract </em><code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.SchemeScorer.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the given list of tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – The list of tokens to weigh.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with the tokens as the keys and the weights as the values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nlp.weighting.SchemeScorer.__weakref__">
<code class="sig-name descname">__weakref__</code><a class="headerlink" href="#nlp.weighting.SchemeScorer.__weakref__" title="Permalink to this definition">¶</a></dt>
<dd><p>list of weak references to the object (if defined)</p>
</dd></dl>

</dd></dl>

<div class="section" id="common-term-weighting-schemes">
<span id="nlp-common"></span><h3>Common Term-Weighting Schemes<a class="headerlink" href="#common-term-weighting-schemes" title="Permalink to this headline">¶</a></h3>
<div class="section" id="module-nlp.weighting.tf">
<span id="term-frequency-tf"></span><h4>Term Frequency (TF)<a class="headerlink" href="#module-nlp.weighting.tf" title="Permalink to this headline">¶</a></h4>
<p>The term frequency weighting scheme is used when there is no need for a global scheme.
The term frequency <span class="math notranslate nohighlight">\(tf_{t,d}\)</span> of a feature <span class="math notranslate nohighlight">\(t\)</span> is equivalent to its frequency <span class="math notranslate nohighlight">\(f_{t,d}\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>:</p>
<div class="math notranslate nohighlight">
\[tf_{t,d} = f_{t,d}\]</div>
<dl class="class">
<dt id="nlp.weighting.tf.TF">
<em class="property">class </em><code class="sig-name descname">TF</code><a class="headerlink" href="#nlp.weighting.tf.TF" title="Permalink to this definition">¶</a></dt>
<dd><p>The term frequency weighting scheme is used when there is no need for a global scheme.
It is an instance of a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> with:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#nlp.weighting.local_schemes.tf.TF" title="nlp.weighting.local_schemes.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> as a local term-weighting scheme, and</p></li>
<li><p><a class="reference internal" href="#nlp.weighting.global_schemes.filler.Filler" title="nlp.weighting.global_schemes.filler.Filler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Filler</span></code></a> as a global term-weighting scheme (that does not change the score at all).</p></li>
</ol>
<dl class="method">
<dt id="nlp.weighting.tf.TF.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.tf.TF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the term-weighting scheme by supplying the TF and filler schemes.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp.weighting.tfidf">
<span id="term-frequency-inverse-document-frequency-tf-idf"></span><h4>Term Frequency-Inverse Document Frequency (TF-IDF)<a class="headerlink" href="#module-nlp.weighting.tfidf" title="Permalink to this headline">¶</a></h4>
<p>The Term Frequency-Inverse Document Frequency (TF-IDF) term-weighting scheme is one of the most popular schemes.
The scheme promotes features that appear commonly in a document, but rarely outside of it.
The TFIDF is simply the multiplication of the <a class="reference internal" href="#nlp.weighting.local_schemes.tf.TF" title="nlp.weighting.local_schemes.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> and <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a> term-weighting schemes:
The weight <span class="math notranslate nohighlight">\(tfidf_{t,d}\)</span> of term <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span> is computed as follows:</p>
<div class="math notranslate nohighlight">
\[tfidf_{t,d} = tf_{t,d} \cdot idf_{t}\]</div>
<p>where <span class="math notranslate nohighlight">\(tf_{t,d}\)</span> is the score assignedd by the <a class="reference internal" href="#nlp.weighting.local_schemes.tf.TF" title="nlp.weighting.local_schemes.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> scheme, and <span class="math notranslate nohighlight">\(idf_{t}\)</span> is the score assigned by the <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a> scheme.</p>
<dl class="class">
<dt id="nlp.weighting.tfidf.TFIDF">
<em class="property">class </em><code class="sig-name descname">TFIDF</code><span class="sig-paren">(</span><em class="sig-param">idf</em>, <em class="sig-param">documents</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.tfidf.TFIDF" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is a quick way of instantiating a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> that represents the Term Frequency-Inverse Document Frequency (TF-IDF) scheme.
It automatically constructs the scheme from the specifications required to create the <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a> scheme.
The result is a <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> with the following components:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#nlp.weighting.local_schemes.tf.TF" title="nlp.weighting.local_schemes.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> as a local term-weighting scheme, and</p></li>
<li><p><a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a> as a global term-weighting scheme (from the given specifications).</p></li>
</ol>
<p>Since this term-weighting scheme is so common, this class provides functionality to export it (using the <a class="reference internal" href="#nlp.weighting.tfidf.TFIDF.to_array" title="nlp.weighting.tfidf.TFIDF.to_array"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_array()</span></code></a> function) and import it (using the <a class="reference internal" href="#nlp.weighting.tfidf.TFIDF.from_array" title="nlp.weighting.tfidf.TFIDF.from_array"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_array()</span></code></a> function) back again.</p>
<dl class="method">
<dt id="nlp.weighting.tfidf.TFIDF.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">idf</em>, <em class="sig-param">documents</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.tfidf.TFIDF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the TF-IDF term-weighting scheme by supplying details about the <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a> scheme.
These include the IDF table and the number of documents in it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idf</strong> (<em>dict</em>) – The IDF table used in conjunction with term weighting.
The keys are the terms, and the corresponding values are the number of documents in which they appear.</p></li>
<li><p><strong>documents</strong> (<em>int</em>) – The number of documents in the IDF table.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.tfidf.TFIDF.to_array">
<code class="sig-name descname">to_array</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.tfidf.TFIDF.to_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Export the TF-IDF as an associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The TF-IDF as an associative array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.tfidf.TFIDF.from_array">
<em class="property">static </em><code class="sig-name descname">from_array</code><span class="sig-paren">(</span><em class="sig-param">array</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.tfidf.TFIDF.from_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an instance of the TF-IDF from the given associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>array</strong> (<em>dict</em>) – The associative array with the attributes to create the TF-IDF.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new instance of the TF-IDF with the same attributes stored in the object.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nlp.weighting.tfidf.TFIDF" title="nlp.weighting.tfidf.TFIDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFIDF</span></code></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="module-nlp.weighting.local_schemes">
<span id="local-term-weighting-schemes"></span><span id="nlp-local"></span><h3>Local Term-Weighting Schemes<a class="headerlink" href="#module-nlp.weighting.local_schemes" title="Permalink to this headline">¶</a></h3>
<p>Local term-weighting schemes assign weights to tokens based on the contents of the document in which they appear.
This type of term-weighting schemes is very simple, and is usually accompanied by <a class="reference internal" href="#nlp-global"><span class="std std-ref">a global term-weighting scheme</span></a>.</p>
<div class="section" id="module-nlp.weighting.local_schemes.boolean">
<span id="boolean"></span><h4>Boolean<a class="headerlink" href="#module-nlp.weighting.local_schemes.boolean" title="Permalink to this headline">¶</a></h4>
<p>The boolean term-weighting scheme is a simple scheme that gives a score of 1 if the term appears in the list of tokens, and 0 otherwise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}bool_{t,d} = \begin{cases}
                             1 &amp; \text{if } t \in d \\
                                 0 &amp; \text{otherwise}
                         \end{cases}\end{split}\]</div>
<dl class="class">
<dt id="nlp.weighting.local_schemes.boolean.Boolean">
<em class="property">class </em><code class="sig-name descname">Boolean</code><a class="headerlink" href="#nlp.weighting.local_schemes.boolean.Boolean" title="Permalink to this definition">¶</a></dt>
<dd><p>The boolean term-weighting scheme is a simple scheme that gives a score of 1 if the term appears in the list of tokens, and 0 otherwise.
In reality, this term-weighting scheme is not aware of all possible tokens.
Therefore it only gives a score of 1 if the term appears in the list of tokens.
The <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> automatically assumes the score of a token is 0 if it is not set.</p>
<dl class="method">
<dt id="nlp.weighting.local_schemes.boolean.Boolean.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.local_schemes.boolean.Boolean.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the given tokens.
The score is 1 if a feature appears in the document, 0 otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – The list of tokens to weigh.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with the tokens as the keys and the weights as the values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id1">
<h4>Term Frequency (TF)<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<span class="target" id="module-nlp.weighting.local_schemes.tf"></span><p>The Term Frequency (TF) local term-weighting scheme assigns a weight according to the number of times that a token appears in the list.
The term frequency <span class="math notranslate nohighlight">\(tf_{t,d}\)</span> of a feature <span class="math notranslate nohighlight">\(t\)</span> is calculated as:</p>
<div class="math notranslate nohighlight">
\[tf_{t,d} = f_{t,d}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{t,d}\)</span> is the frequency of token <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>.</p>
<dl class="class">
<dt id="nlp.weighting.local_schemes.tf.TF">
<em class="property">class </em><code class="sig-name descname">TF</code><a class="headerlink" href="#nlp.weighting.local_schemes.tf.TF" title="Permalink to this definition">¶</a></dt>
<dd><p>The Term Frequency (TF) term-weighting scheme is a simple term weighting schemes that assigns a weight equal to the number of times that the feature appears in a document.</p>
<dl class="method">
<dt id="nlp.weighting.local_schemes.tf.TF.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.local_schemes.tf.TF.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the given tokens.
The score is equal to the frequency of the token in the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – The list of tokens to weigh.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with the tokens as the keys and the weights as the values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="module-nlp.weighting.global_schemes">
<span id="global-term-weighting-schemes"></span><span id="nlp-global"></span><h3>Global Term-Weighting Schemes<a class="headerlink" href="#module-nlp.weighting.global_schemes" title="Permalink to this headline">¶</a></h3>
<p>Global term-weighting schemes assign weights to tokens based on the contents of a corpus, and not just the document they appear appear in.
This kind of term-weighting schemes is usually accompanied by <a class="reference internal" href="#nlp-local"><span class="std std-ref">a local term-weighting scheme</span></a>.</p>
<div class="section" id="module-nlp.weighting.global_schemes.filler">
<span id="filler"></span><h4>Filler<a class="headerlink" href="#module-nlp.weighting.global_schemes.filler" title="Permalink to this headline">¶</a></h4>
<p>The filler global term-weighting scheme is used when there is no need for a global term-weighting scheme.
The scheme assigns a score of 1 to all tokens such that it does not influence their overall score assigned by the <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a>.</p>
<dl class="class">
<dt id="nlp.weighting.global_schemes.filler.Filler">
<em class="property">class </em><code class="sig-name descname">Filler</code><a class="headerlink" href="#nlp.weighting.global_schemes.filler.Filler" title="Permalink to this definition">¶</a></dt>
<dd><p>The filler global term-weighting scheme is used when there is no need for a global term-weighting scheme because it does not change the overall weight of tokens.</p>
<dl class="method">
<dt id="nlp.weighting.global_schemes.filler.Filler.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">tokens</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.filler.Filler.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Give a constant score the given tokens.
The chosen constant is 1 so that the <a class="reference internal" href="#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> is not influenced at al..</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – The list of tokens to weigh.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with the tokens as the keys and the weights as the values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp.weighting.global_schemes.idf">
<span id="inverse-document-frequency-idf"></span><h4>Inverse Document Frequency (IDF)<a class="headerlink" href="#module-nlp.weighting.global_schemes.idf" title="Permalink to this headline">¶</a></h4>
<p>The Inverse Document Frequency (IDF) global term-weighting scheme penalizes common terms when assigning weight.
The reasoning is that tokens that are very common in a corpus are not informative generally.
On the other hand, terms that appear often in one document but rarely outside characterize that document.</p>
<p>The IDF <span class="math notranslate nohighlight">\(idf_{t}\)</span> for term <span class="math notranslate nohighlight">\(t\)</span> is computed as follows:</p>
<div class="math notranslate nohighlight">
\[idf_{t} = \log{\frac{N}{n_t}}\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the total number of documents in the corpus, and <span class="math notranslate nohighlight">\(n_t\)</span> is the total number of documents that contain term <span class="math notranslate nohighlight">\(t\)</span>.</p>
<dl class="class">
<dt id="nlp.weighting.global_schemes.idf.IDF">
<em class="property">class </em><code class="sig-name descname">IDF</code><span class="sig-paren">(</span><em class="sig-param">idf</em>, <em class="sig-param">documents</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF" title="Permalink to this definition">¶</a></dt>
<dd><p>The Inverse Document Frequency (IDF) is one of the most common term-weighting schemes.
This scheme promotes tokens that are common in one document, and uncommon in the rest of the corpus.
As part of the calculation, the IDF scheme needs two pieces of information:</p>
<ol class="arabic simple">
<li><p>The number of documents in which tokens appear, and</p></li>
<li><p>The number of documents in the corpus.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to mitigate for tokens that are not present in the corpus, this class uses Laplace smoothing.</p>
</div>
<p>In addition to the standard <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF.score" title="nlp.weighting.global_schemes.idf.IDF.score"><code class="xref py py-func docutils literal notranslate"><span class="pre">score()</span></code></a> function, this class also the <a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF.from_documents" title="nlp.weighting.global_schemes.idf.IDF.from_documents"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_documents()</span></code></a> function.
This function receives a corpus and constructs the IDF table from it.
The number of documents in the corpus, which is required by this class, can be extracted trivially.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idf</strong> – The IDF table used in conjunction with term weighting.</p></li>
<li><p><a class="reference internal" href="summarization.html#summarization.summary.Summary.documents" title="summarization.summary.Summary.documents"><strong>documents</strong></a> (<em>int</em>) – The number of documents in the corpus.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nlp.weighting.global_schemes.idf.IDF.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">idf</em>, <em class="sig-param">documents</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the term-weighting scheme with the IDF table and the number of documents in that scheme.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idf</strong> (<em>dict</em>) – The IDF table used in conjunction with term weighting.
The keys are the terms, and the corresponding values are the number of documents in which they appear.</p></li>
<li><p><strong>documents</strong> (<em>int</em>) – The number of documents in the IDF table.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – When the document frequency of a term is higher than the number of the IDF documents.</p></li>
<li><p><strong>ValueError</strong> – When the document frequency of a term is negative.</p></li>
<li><p><strong>ValueError</strong> – When the number of documents is negative.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.global_schemes.idf.IDF.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">tokens</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the given tokens based on the number of documents they appear.
Tokens that appear in many documents receive a low score, whereas those that do not receive a high score.</p>
<p>Since the denominator is 0 if the token does not appear in the corpus, this function uses Laplace smoothing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list of str</em>) – The list of tokens to weigh.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with the tokens as the keys and the weights as the values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.global_schemes.idf.IDF.to_array">
<code class="sig-name descname">to_array</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF.to_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Export the IDF as an associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The IDF as an associative array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.global_schemes.idf.IDF.from_array">
<em class="property">static </em><code class="sig-name descname">from_array</code><span class="sig-paren">(</span><em class="sig-param">array</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF.from_array" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an instance of the IDF from the given associative array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>array</strong> (<em>dict</em>) – The associative array with the attributes to create the IDF.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A new instance of the IDF with the same attributes stored in the object.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#nlp.weighting.global_schemes.idf.IDF" title="nlp.weighting.global_schemes.idf.IDF"><code class="xref py py-class docutils literal notranslate"><span class="pre">IDF</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.weighting.global_schemes.idf.IDF.from_documents">
<em class="property">static </em><code class="sig-name descname">from_documents</code><span class="sig-paren">(</span><em class="sig-param">documents</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.weighting.global_schemes.idf.IDF.from_documents" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the IDF table from the given set of documents.
This function extracts all unique terms from the documents’ dimensions and counts the number of documents in which they appear at least once.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not return the number of documents, but only the IDF table.
The number of documents can be extracted easily using the <code class="xref py py-func docutils literal notranslate"><span class="pre">len()</span></code> function.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>documents</strong> (list of <a class="reference internal" href="#nlp.document.Document" title="nlp.document.Document"><code class="xref py py-class docutils literal notranslate"><span class="pre">Document</span></code></a>) – The documents from which the IDF table will be created.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary, where the keys are the document tokens and the values are their document frequency.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</div>
<div class="section" id="module-nlp.cleaners">
<span id="cleaners"></span><h2>Cleaners<a class="headerlink" href="#module-nlp.cleaners" title="Permalink to this headline">¶</a></h2>
<p>Document cleaners are classes that improve the presentation of text.
Cleaning can be used in summarization, for example, when the output text may be noisy or informal (such as when working with tweets).
However, it can also be used as a pre-processing step to remove undesired fragments.</p>
<p>The <a class="reference internal" href="#nlp.cleaners.Cleaner" title="nlp.cleaners.Cleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Cleaner</span></code></a> is a base class for all cleaners.
However, it also performs simple processing on text to remove common mistakes, such as missing punctuation or multiple contiguous white-spaces.</p>
<p>All cleaners are configured in their constructor to ensure that cleaning is uniform across all documents.
The main functionality resides in the <a class="reference internal" href="#nlp.cleaners.Cleaner.clean" title="nlp.cleaners.Cleaner.clean"><code class="xref py py-func docutils literal notranslate"><span class="pre">clean()</span></code></a> function, which receives text and returns a cleaner version of it.
Other cleaners that inherit the <a class="reference internal" href="#nlp.cleaners.Cleaner" title="nlp.cleaners.Cleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Cleaner</span></code></a> class would likely need to change this function.</p>
<dl class="class">
<dt id="nlp.cleaners.Cleaner">
<em class="property">class </em><code class="sig-name descname">Cleaner</code><span class="sig-paren">(</span><em class="sig-param">remove_alt_codes=False</em>, <em class="sig-param">complete_sentences=False</em>, <em class="sig-param">collapse_new_lines=False</em>, <em class="sig-param">collapse_whitespaces=False</em>, <em class="sig-param">capitalize_first=False</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.Cleaner" title="Permalink to this definition">¶</a></dt>
<dd><p>The base cleaner is meant to perform only basing pre-processing and cleaning.
These functions are generally applicable to any type of text.</p>
<p>You need to specify all cleaner settings in the constructor.
This design is purposeful so that the cleaner processes all text in the same way.
By default, the cleaner only removes whitespaces or tabs at the start and end of the text.</p>
<p>After creating a cleaner, use the <a class="reference internal" href="#nlp.cleaners.Cleaner.clean" title="nlp.cleaners.Cleaner.clean"><code class="xref py py-func docutils literal notranslate"><span class="pre">clean()</span></code></a> function to clean the text.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>remove_alt_codes</strong> (<em>bool</em>) – A boolean indicating whether alt-codes should be removed.</p></li>
<li><p><strong>complete_sentences</strong> (<em>bool</em>) – A boolean indicating whether the sentences should be completed.
The cleaner cannot identify any incomplete sentences in the middle of the text, so it only completes the last sentence.</p></li>
<li><p><strong>collapse_new_lines</strong> (<em>bool</em>) – A boolean indicating whether new lines should be collapsed into whitespaces.</p></li>
<li><p><strong>collapse_whitespaces</strong> (<em>bool</em>) – A boolean indicating whether consecutive whitespaces and tabs should be collapsed into a single whitespace.
This also removes whitespaces just before periods.</p></li>
<li><p><strong>capitalize_first</strong> (<em>bool</em>) – A boolean indicating whether to capitalize the first letter.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nlp.cleaners.Cleaner.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">remove_alt_codes=False</em>, <em class="sig-param">complete_sentences=False</em>, <em class="sig-param">collapse_new_lines=False</em>, <em class="sig-param">collapse_whitespaces=False</em>, <em class="sig-param">capitalize_first=False</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.Cleaner.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the cleaner with the basic configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>remove_alt_codes</strong> (<em>bool</em>) – A boolean indicating whether alt-codes should be removed.</p></li>
<li><p><strong>complete_sentences</strong> (<em>bool</em>) – A boolean indicating whether the sentences should be completed.
The cleaner cannot identify any incomplete sentences in the middle of the text, so it only completes the last sentence.</p></li>
<li><p><strong>collapse_new_lines</strong> (<em>bool</em>) – A boolean indicating whether new lines should be collapsed into whitespaces.</p></li>
<li><p><strong>collapse_whitespaces</strong> (<em>bool</em>) – A boolean indicating whether consecutive whitespaces and tabs should be collapsed into a single whitespace.
This also removes whitespaces just before periods.</p></li>
<li><p><strong>capitalize_first</strong> (<em>bool</em>) – A boolean indicating whether to capitalize the first letter.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.cleaners.Cleaner.clean">
<code class="sig-name descname">clean</code><span class="sig-paren">(</span><em class="sig-param">text</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.Cleaner.clean" title="Permalink to this definition">¶</a></dt>
<dd><p>Clean the given text based on the parameters specified when creating the cleaner.
The basic cleaner always strips empty whitespaces before and after all processing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The text to clean.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The cleaned text.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="nlp.cleaners.Cleaner.__weakref__">
<code class="sig-name descname">__weakref__</code><a class="headerlink" href="#nlp.cleaners.Cleaner.__weakref__" title="Permalink to this definition">¶</a></dt>
<dd><p>list of weak references to the object (if defined)</p>
</dd></dl>

</dd></dl>

<div class="section" id="module-nlp.cleaners.tweet_cleaner">
<span id="tweet-cleaner"></span><h3>Tweet Cleaner<a class="headerlink" href="#module-nlp.cleaners.tweet_cleaner" title="Permalink to this headline">¶</a></h3>
<p>The tweet cleaner builds on the base cleaner, but adds functionality that is specific to Twitter and other informal text in general.
The Twitter-specific functionality involves retweet prefixes (<cite>RT</cite> at the start of the tweet), hashtags (such as <cite>#EvenTDT</cite>) and mentions (like <cite>&#64;NicholasMamo</cite>).
The tweet cleaner is also capable of removing unicode entities, like certain emojis, and URLs.</p>
<dl class="class">
<dt id="nlp.cleaners.tweet_cleaner.TweetCleaner">
<em class="property">class </em><code class="sig-name descname">TweetCleaner</code><span class="sig-paren">(</span><em class="sig-param">remove_unicode_entities=False</em>, <em class="sig-param">remove_urls=False</em>, <em class="sig-param">remove_hashtags=False</em>, <em class="sig-param">split_hashtags=False</em>, <em class="sig-param">remove_retweet_prefix=False</em>, <em class="sig-param">replace_mentions=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.tweet_cleaner.TweetCleaner" title="Permalink to this definition">¶</a></dt>
<dd><p>The tweet cleaner removes needless information from a text to make it presentable.
This includes Twitter-specific syntax and URLs.</p>
<p>By default, the tweet cleaner only strips the text of whitespaces at the beginning or at the end of text.
You can add other cleaning steps using the appropriate parameters when creating a <a class="reference internal" href="#nlp.cleaners.tweet_cleaner.TweetCleaner" title="nlp.cleaners.tweet_cleaner.TweetCleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">TweetCleaner</span></code></a>.
In addition, you can also specify additional parameters from the base <a class="reference internal" href="#nlp.cleaners.Cleaner" title="nlp.cleaners.Cleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Cleaner</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>remove_unicode_entities</strong> (<em>bool</em>) – A boolean indicating whether unicode entities should be removed.
Note that this also includes emojis.</p></li>
<li><p><strong>remove_urls</strong> (<em>bool</em>) – A boolean indicating whether URLs should be removed.</p></li>
<li><p><strong>remove_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags that cannot be split should be removed.</p></li>
<li><p><strong>split_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags should be split.</p></li>
<li><p><strong>remove_retweet_prefix</strong> (<em>bool</em>) – A boolean indicating whether the retweet prefix should be removed.</p></li>
<li><p><strong>replace_mentions</strong> (<em>bool</em>) – A boolean indicating whether mentions should be replaced with the user’s display name.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nlp.cleaners.tweet_cleaner.TweetCleaner.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">remove_unicode_entities=False</em>, <em class="sig-param">remove_urls=False</em>, <em class="sig-param">remove_hashtags=False</em>, <em class="sig-param">split_hashtags=False</em>, <em class="sig-param">remove_retweet_prefix=False</em>, <em class="sig-param">replace_mentions=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.tweet_cleaner.TweetCleaner.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the tweet cleaner by specifying the type of cleaning operations it should perform.
This design is purposeful so that the tweet cleaner processes all text in the same way.</p>
<p>The same configuration accepted by the <a class="reference internal" href="#nlp.cleaners.Cleaner" title="nlp.cleaners.Cleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Cleaner</span></code></a> are accepted as arguments and keyword arguments.
They are then passed on to the parent constructor, <a class="reference internal" href="#nlp.cleaners.Cleaner.__init__" title="nlp.cleaners.Cleaner.__init__"><code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>remove_unicode_entities</strong> (<em>bool</em>) – A boolean indicating whether unicode entities should be removed.
Note that this also includes emojis.</p></li>
<li><p><strong>remove_urls</strong> (<em>bool</em>) – A boolean indicating whether URLs should be removed.</p></li>
<li><p><strong>remove_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags that cannot be split should be removed.</p></li>
<li><p><strong>split_hashtags</strong> (<em>bool</em>) – A boolean indicating whether hashtags should be split.</p></li>
<li><p><strong>remove_retweet_prefix</strong> (<em>bool</em>) – A boolean indicating whether the retweet prefix should be removed.</p></li>
<li><p><strong>replace_mentions</strong> (<em>bool</em>) – A boolean indicating whether mentions should be replaced with the user’s display name.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nlp.cleaners.tweet_cleaner.TweetCleaner.clean">
<code class="sig-name descname">clean</code><span class="sig-paren">(</span><em class="sig-param">text</em>, <em class="sig-param">tweet=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nlp.cleaners.tweet_cleaner.TweetCleaner.clean" title="Permalink to this definition">¶</a></dt>
<dd><p>Clean the given text.
The basic cleaner always strips empty whitespaces before any pre-processing.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tweet</span></code> parameter is optional, and is only required when replacing tweet mentions by the respective user’s screen name (for example, replacing <cite>&#64;NicholasMamo</cite> to <cite>Nicholas Mamo</cite>).
In this case, the function requires the original <code class="docutils literal notranslate"><span class="pre">tweet</span></code> object to look for the <code class="docutils literal notranslate"><span class="pre">user</span></code> object, which contains details for replacing the username.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – The text to clean.</p></li>
<li><p><strong>tweet</strong> (<em>dict</em>) – The tweet to use to clean the tweet.
This is only used when mentioned users’ names are replaced in the text.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The cleaned text.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="wikinterface.html" class="btn btn-neutral float-right" title="4. Wikinterface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vsm.html" class="btn btn-neutral float-left" title="2. Vector Space Model (VSM)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Nicholas Mamo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>