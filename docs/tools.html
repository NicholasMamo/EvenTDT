

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta content="Tools to collect corpora and create timelines out of events." name="description" />
<meta content="Python, TDT, events, Twitter" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. Tools &mdash; EvenTDT 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/eventdt.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. Vector Space Model (VSM)" href="vsm.html" />
    <link rel="prev" title="0. Configuration" href="config.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> EvenTDT
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="config.html">0. Configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. Tools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-tools.collect">Data Collection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-processing">Pre-processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-tools.tokenizer">Tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-tools.idf">IDF construction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#automatic-term-extraction">Automatic Term Extraction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-tools.terms">Term extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-tools.bootstrap">Bootstrapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-tools.correlation">Correlation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-tools.participants">Automatic Participant Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-tools.consume">Topic Detection and Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-tools.summarize">Summarization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="vsm.html">2. Vector Space Model (VSM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">3. Natural Language Processing (NLP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="wikinterface.html">4. Wikinterface</a></li>
<li class="toctree-l1"><a class="reference internal" href="apd.html">5. Automatic Participant Detection (APD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tdt.html">6. Topic Detection and Tracking (TDT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="summarization.html">7. Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="twitter.html">8. Twitter</a></li>
<li class="toctree-l1"><a class="reference internal" href="consumers.html">9. Consumers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml.html">10. Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ate.html">11. Automatic Term Extraction (ATE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="other.html">12. Other</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">EvenTDT</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>1. Tools</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/tools.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
    <div class="section" id="tools">
<h1>1. Tools<a class="headerlink" href="#tools" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-tools"></span><p>EvenTDT comes with tools to help you collect corpora from Twitter and create timelines from them.
Keep reading to learn more about the different tools available in EvenTDT.</p>
<div class="section" id="module-tools.collect">
<span id="data-collection"></span><h2>Data Collection<a class="headerlink" href="#module-tools.collect" title="Permalink to this headline">¶</a></h2>
<p>A tool to collect tweets.</p>
<p>The tool collects a corpus of tweets.
A corpus can be split into an understanding and an event corpus.
Both can be collected by specifying tracking keywords.
All related corpora are stored together in the same directory.
Each corpus is a JSON file, where each line is one tweet.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/collect.py <span class="se">\</span>
    -o data/#ARSWAT <span class="se">\</span>
    -t <span class="s1">&#39;#ARSWAT&#39;</span> Arsenal Watford <span class="se">\</span>
    --understanding <span class="m">60</span> <span class="se">\</span>
    --event <span class="m">60</span>
</pre></div>
</div>
<p>If no tracking keywords are specified, a sample of all tweets is collected.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/collect.py <span class="se">\</span>
    -o data/sample <span class="se">\</span>
    --understanding <span class="m">60</span> <span class="se">\</span>
    --event <span class="m">60</span>
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>                       <em>&lt;Required&gt;</em> The data directory where the corpus should be written.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">--track</span></code>                        <em>&lt;Optional&gt;</em> A list of tracking keywords. If none are given, the sample stream is used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-u</span> <span class="pre">--understanding</span></code>        <em>&lt;Optional&gt;</em> The length of the understanding period in minutes. If it is not given, the understanding period is skipped.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-e</span> <span class="pre">--event</span></code>                        <em>&lt;Optional&gt;</em> The length of the event period in minutes. If it is not given, the event period is skipped.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">--account</span></code>                      <em>&lt;Optional&gt;</em> The account to use to collect the corpus with, as an index of the configuration’s accounts. Defaults to the first account.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="pre-processing">
<h2>Pre-processing<a class="headerlink" href="#pre-processing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-tools.tokenizer">
<span id="tokenizer"></span><h3>Tokenizer<a class="headerlink" href="#module-tools.tokenizer" title="Permalink to this headline">¶</a></h3>
<p>A tool to tokenize a corpus of tweets.
The tokenizer can be used to pre-process a corpus.</p>
<p>Each line in the tokenizer corresponds to a tweet.
Each line is a JSON object containing, at minimum, the tweet ID, the text used for the tokenization and the tokens.</p>
<p>Part-of-speech extraction can be specified by using the <cite>–nouns</cite>, <cite>–proper-nouns</cite>, <cite>–verbs</cite> and <cite>–adjectives</cite> arguments.
If none are given, all tokens are collected, including other parts-of-speech, like adverbs.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/tokenizer.py <span class="se">\</span>
    -f data/sample.json <span class="se">\</span>
    -o data/idf.json <span class="se">\</span>
    --remove-unicode-entities <span class="se">\</span>
    --remove-stopwords <span class="se">\</span>
    --normalize-words --stem
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--file</span></code>                                                 <em>&lt;Required&gt;</em> The file to use to construct the tokenized corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>                                               <em>&lt;Required&gt;</em> The file where to save the tokenized corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-k</span> <span class="pre">--keep</span></code>                                                 <em>&lt;Optional&gt;</em> The tweet attributes to store.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--remove-retweets</span></code>                                 <em>&lt;Optional&gt;</em> Exclude retweets from the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--remove-unicode-entities</span></code>                 <em>&lt;Optional&gt;</em> Remove unicode entities from the tweets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--normalize-words</span></code>                                 <em>&lt;Optional&gt;</em> Normalize words with repeating characters in them.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--character-normalization-count</span></code>   <em>&lt;Optional&gt;</em> The number of times a character must repeat for it to be normalized. Used only with the <code class="docutils literal notranslate"><span class="pre">--normalize-words</span></code> flag.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--remove-stopwords</span></code>                                <em>&lt;Optional&gt;</em> Remove stopwords from the tokens.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-stem</span></code>                                                             <em>&lt;Optional&gt;</em> Stem the tokens when constructing the tokenized corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nouns</span></code>                                                   <em>&lt;Optional&gt;</em> Extract nouns from the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--proper-nouns</span></code>                                    <em>&lt;Optional&gt;</em> Extract proper nouns from the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--verbs</span></code>                                                   <em>&lt;Optional&gt;</em> Extract verbs from the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--adjectives</span></code>                                              <em>&lt;Optional&gt;</em> Extract adjectives from the corpus.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-tools.idf">
<span id="idf-construction"></span><h3>IDF construction<a class="headerlink" href="#module-tools.idf" title="Permalink to this headline">¶</a></h3>
<p>A tool to create a TF-IDF scheme from a corpus of tweets.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/idf.py <span class="se">\</span>
    -f data/sample.json <span class="se">\</span>
    -o data/idf.json <span class="se">\</span>
    --remove-unicode-entities <span class="se">\</span>
    --normalize-words --stem
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--file</span></code>                                                 <em>&lt;Required&gt;</em> The file to use to construct the TF-IDF scheme.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>                                               <em>&lt;Required&gt;</em> The file where to save the TF-IDF scheme.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--remove-retweets</span></code>                                 <em>&lt;Optional&gt;</em> Exclude retweets from the corpus.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--remove-unicode-entities</span></code>                 <em>&lt;Optional&gt;</em> Remove unicode entities from the TF-IDF scheme.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--normalize-words</span></code>                                 <em>&lt;Optional&gt;</em> Normalize words with repeating characters in them.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--character-normalization-count</span></code>   <em>&lt;Optional&gt;</em> The number of times a character must repeat for it to be normalized. Used only with the <code class="docutils literal notranslate"><span class="pre">--normalize-words</span></code> flag.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-stem</span></code>                                                             <em>&lt;Optional&gt;</em> Stem the tokens when constructing the TF-IDF scheme.</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="automatic-term-extraction">
<h2>Automatic Term Extraction<a class="headerlink" href="#automatic-term-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-tools.terms">
<span id="term-extraction"></span><h3>Term extraction<a class="headerlink" href="#module-tools.terms" title="Permalink to this headline">¶</a></h3>
<p>A tool to automatically extract terms from the given corpora.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/terms.py <span class="se">\</span>
    -f data/tokenized_corpus.json <span class="se">\</span>
    -m tfidf <span class="se">\</span>
    --tfidf data/idf.json <span class="se">\</span>
    -o data/bootstrapped.json
</pre></div>
</div>
<p>Apart from providing the main method to extract terms, it is possible to pass on a re-ranker method.
The re-ranker extracts terms separately.
This tool multiplies the ATE method’s scores with the re-ranker term’s scores.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/terms.py <span class="se">\</span>
    -f data/tokenized_corpus.json <span class="se">\</span>
    -m tfidf <span class="se">\</span>
    --tfidf data/idf.json <span class="se">\</span>
    -o data/bootstrapped.json
    --reranker entropy <span class="se">\</span>
    --reranker-files data/idf.json
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--files</span></code>                <em>&lt;Required&gt;</em> The input corpora from where to extract domain-specific terms.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">--method</span></code>               <em>&lt;Required&gt;</em> The method to use to look for similar keywords; supported: <cite>TF</cite>, <cite>TFIDF</cite>, <cite>Rank</cite>, <cite>Specificity</cite>, <cite>TFDCF</cite>, <cite>EFIDF</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>               <em>&lt;Required&gt;</em> The path to the file where to store the extracted terms.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-r</span> <span class="pre">--reranker</span></code>             <em>&lt;Optional&gt;</em> The method to use to re-rank terms; supported: <cite>Entropy</cite>, <cite>Variability</cite>; defaults to no re-ranking.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tfidf</span></code>                   <em>&lt;Optional&gt;</em> The TF-IDF scheme to use to extract terms (used only with the <cite>TF-IDF</cite> method).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--general</span></code>                 <em>&lt;Optional&gt;</em> A path or paths to general corpora used for comparison with the domain-specific corpora (used only with the <cite>Rank</cite>, <cite>Specificity</cite> and <cite>TF-DCF</cite> methods).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--cutoff</span></code>                  <em>&lt;Optional&gt;</em> The minimum term frequency to consider when ranking terms (used only with the <cite>Specificity</cite> method).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--base</span></code>                    <em>&lt;Optional&gt;</em> The logarithmic base (used only with the <cite>EF-IDF</cite> method).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--reranker-base</span></code>   <em>&lt;Optional&gt;</em> The logarithmic base (used only with the <cite>Variability</cite> and <cite>Entropy</cite> re-rankers); defaults to 10.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--reranker-files</span></code>  <em>&lt;Optional&gt;</em> The input corpora to use for the re-ranker.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-tools.bootstrap">
<span id="bootstrapping"></span><h3>Bootstrapping<a class="headerlink" href="#module-tools.bootstrap" title="Permalink to this headline">¶</a></h3>
<p>A tool that receives a seed set of terms and looks for similar terms in the given corpora.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/bootstrap.py <span class="se">\</span>
    -s data/seed.txt <span class="se">\</span>
    -c data/candidates.txt <span class="se">\</span>
    -o data/bootstrapped.json <span class="se">\</span>
    -f data/tokenized_corpus.json
</pre></div>
</div>
<p>The seed and candidates files can be either text files or the output from the <code class="docutils literal notranslate"><span class="pre">terms</span></code> tool.
If a text file is given, this tool expects one word on each line.</p>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-s</span> <span class="pre">--seed</span></code>                 <em>&lt;Required&gt;</em> The path to the file containing seed keywords, expected to contain one keyword on each line. Alternatively, the output from the <code class="docutils literal notranslate"><span class="pre">terms</span></code> tool can be provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--files</span></code>                <em>&lt;Required&gt;</em> The input corpora where to look for similar keywords, expected to be already tokenized by the <cite>tokenize</cite> tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">--method</span></code>               <em>&lt;Required&gt;</em> The method to use to look for similar keywords; supported: <cite>CHI</cite>, <cite>Log</cite>, <cite>PMI</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>               <em>&lt;Required&gt;</em> The path to the file where to store the bootstrapped keywords.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">--candidates</span></code>   <em>&lt;Optional&gt;</em> The path to the file containing candidate keywords, expected to contain one keyword on each line. Alternatively, the output from the <code class="docutils literal notranslate"><span class="pre">terms</span></code> tool can be provided. If no candidates are given, all vocabulary keywords are considered candidates.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-i</span> <span class="pre">--iterations</span></code>   <em>&lt;Optional&gt;</em> The number of iterations to spend bootstrapping; defaults to 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-k</span> <span class="pre">--keep</span></code>                 <em>&lt;Optional&gt;</em> The number of keywords to keep after each iteration; defaults to 5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--generate</span></code>                <em>&lt;Optional&gt;</em> The number of candidate keywords to generate if no candidates are provided; defaults to 100.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-seed</span></code>                <em>&lt;Optional&gt;</em> The number of seed words to use from the given files; defaults to all words.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-candidates</span></code>  <em>&lt;Optional&gt;</em> The number of candidate words to use from the given files; defaults to all words.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-tools.correlation">
<span id="correlation"></span><h3>Correlation<a class="headerlink" href="#module-tools.correlation" title="Permalink to this headline">¶</a></h3>
<p>The correlation tool receives a list of words and a list of files and computes their correlation.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/correlation.py <span class="se">\</span>
    --terms first half
    --files data/tokenized_corpus.json <span class="se">\</span>
    --method CHI <span class="se">\</span>
    --output data/correlation.json
</pre></div>
</div>
<p>If the terms are stored in a file produced by the <code class="docutils literal notranslate"><span class="pre">terms</span></code> or <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code> scripts, you can load them as follows:</p>
<blockquote>
<div><p>./tools/correlation.py –terms data/terms.json –files data/tokenized_corpus.json –method CHI –output data/correlation.json</p>
</div></blockquote>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">--terms</span></code>                <em>&lt;Required&gt;</em> A list of terms, or the path to the file containing a list of terms for which to calculate the correlation. It can be the output from the <code class="docutils literal notranslate"><span class="pre">terms</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code> tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--files</span></code>                <em>&lt;Required&gt;</em> The input corpora from which to calculate the correlation betwee terms, expected to be already tokenized by the <cite>tokenize</cite> tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">--method</span></code>               <em>&lt;Required&gt;</em> The method to use to compute the correlation values; supported: <cite>PMI</cite>, <cite>CHI</cite>, <cite>Log</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>               <em>&lt;Required&gt;</em> The path to the file where to store the correlation values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-terms</span></code>               <em>&lt;Optional&gt;</em> The maximum number of terms to use, useful when loading terms from files; defaults to all terms.</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="module-tools.participants">
<span id="automatic-participant-detection"></span><h2>Automatic Participant Detection<a class="headerlink" href="#module-tools.participants" title="Permalink to this headline">¶</a></h2>
<p>A tool to automatically extract participants from the given corpora.
This tool is meant to run on the understanding corpora to extract the event’s participants.</p>
<p>To use a ready-made model, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/participants.py <span class="se">\</span>
    -f data/understanding.json <span class="se">\</span>
    --clean <span class="se">\</span>
    --model ELDParticipantDetector <span class="se">\</span>
    --tfidf data/idf.json <span class="se">\</span>
    -o data/participants.json
</pre></div>
</div>
<p>Alternatively, you can create your own model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/participants.py <span class="se">\</span>
    -f data/understanding.json <span class="se">\</span>
    --clean <span class="se">\</span>
    --extractor EntityExtractor <span class="se">\</span>
    --scorer TFScorer <span class="se">\</span>
    --filter RankFilter -k <span class="m">10</span> <span class="se">\</span>
    -o data/participants.json
</pre></div>
</div>
<p>You can modify an existing model by providing the components yourself.
For example, this snippet uses the <a class="reference internal" href="apd.html#apd.extractors.local.entity_extractor.EntityExtractor" title="apd.extractors.local.entity_extractor.EntityExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntityExtractor</span></code></a> instead of the default <a class="reference internal" href="apd.html#apd.extractors.local.twitterner_entity_extractor.TwitterNEREntityExtractor" title="apd.extractors.local.twitterner_entity_extractor.TwitterNEREntityExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TwitterNEREntityExtractor</span></code></a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/participants.py <span class="se">\</span>
    -f data/understanding.json <span class="se">\</span>
    --clean <span class="se">\</span>
    --model ELDParticipantDetector <span class="se">\</span>
    --tfidf data/idf.json <span class="se">\</span>
    --extractor EntityExtractor <span class="se">\</span>
    -o data/participants.json
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--file</span></code>                 <em>&lt;Required&gt;</em> The input corpus from where to extract participants.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>               <em>&lt;Required&gt;</em> The path to the file where to store the extracted participants.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--clean</span></code>                   <em>&lt;Optional&gt;</em> Clean the tweets before extracting participants. This replaces tweet mentions with the display name using the <a class="reference internal" href="nlp.html#nlp.cleaners.tweet_cleaner.TweetCleaner" title="nlp.cleaners.tweet_cleaner.TweetCleaner"><code class="xref py py-class docutils literal notranslate"><span class="pre">TweetCleaner</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">--model</span></code>                <em>&lt;Optional&gt;</em> The type of model to use; supported: <cite>ELDParticipantDetector</cite>; defaults to a normal participant detector.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--extractor</span></code>               <em>&lt;Optional&gt;</em> The extractor to use to extract candidate participants; supported: <cite>EntityExtractor</cite> (default), <cite>TokenExtractor</cite>, <cite>TwitterNEREntityExtractor</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--scorer</span></code>                  <em>&lt;Optional&gt;</em> The scorer to use to score candidate participants; supported: <cite>TFScorer</cite> (default), <cite>DFScorer</cite>, <cite>LogDFScorer</cite>, <cite>LogTFScorer</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--filter</span></code>                  <em>&lt;Optional&gt;</em> The filter to use to filter candidate participants; supported: <cite>RankFilter</cite>, <cite>ThresholdFilter</cite>; defaults to no filter.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-k</span></code>                                <em>&lt;Optional&gt;</em> The number of candidates to retain when filtering candidates (used only with the <cite>RankFilter</cite>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--threshold</span></code>               <em>&lt;Optional&gt;</em> The score threshold to use when filtering candidates (used only with the <cite>ThresholdFilter</cite>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tfidf</span></code>                   <em>&lt;Optional&gt;</em> The TF-IDF scheme to use when creating documents (used only with the <cite>ELDParticipantDetector</cite> model).</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-tools.consume">
<span id="topic-detection-and-tracking"></span><h2>Topic Detection and Tracking<a class="headerlink" href="#module-tools.consume" title="Permalink to this headline">¶</a></h2>
<p>The consumer receives an input file and consumes it with one of the given consumers.
This consumer is split into two asynchronous tasks.
The first task reads the file, and the second consumes it.</p>
<p>If an understanding file is provided, it is used for the understanding task.
The process is similar to before, with two asynchronous tasks.
The first task reads the file, and the second consumes it, this time to understand the event.</p>
<p>All dataset files are expected to contain one tweet on every line, encoded as JSON strings.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/consume.py <span class="se">\</span>
    -f data/event/event.json <span class="se">\</span>
    -c PrintConsumer
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--file</span></code>                                 <em>&lt;Required&gt;</em> The file to consume.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">--class</span></code>                                <em>&lt;Required&gt;</em> The consumer to use; supported: <cite>ELDConsumer</cite>, <cite>FIREConsumer</cite>, <cite>PrintConsumer</cite>, <cite>StatConsumer</cite>, <cite>ZhaoConsumer</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-u</span> <span class="pre">--understanding</span></code>                <em>&lt;Optional&gt;</em> The understanding file used to understand the event.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-s</span> <span class="pre">--speed</span></code>                                <em>&lt;Optional&gt;</em> The speed at which the file is consumed, defaults to 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--skip</span></code>                                    <em>&lt;Optional&gt;</em> The amount of time to skip from the beginning of the file in minutes, defaults to 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-inactivity</span></code>                  <em>&lt;Optional&gt;</em> The maximum time in seconds to wait for new tweets to arrive before stopping, defaults to 60 seconds.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-time</span></code>                                <em>&lt;Optional&gt;</em> The maximum time in minutes to spend reading the corpus, indefinite if it is less than 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no-cache</span></code>                                <em>&lt;Optional&gt;</em> If specified, the cached understanding is not used. The new understanding is cached instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--scheme</span></code>                                  <em>&lt;Optional&gt;</em> If specified, the path to the <a class="reference internal" href="nlp.html#nlp.weighting.TermWeightingScheme" title="nlp.weighting.TermWeightingScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">TermWeightingScheme</span></code></a> to use. If it is not specified, the <a class="reference internal" href="nlp.html#nlp.weighting.tf.TF" title="nlp.weighting.tf.TF"><code class="xref py py-class docutils literal notranslate"><span class="pre">TF</span></code></a> scheme is used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--min-size</span></code>                                <em>&lt;Optional&gt;</em> The minimum number of tweets in a cluster to consider it as a candidate topic, defaults to 3.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--threshold</span></code>                               <em>&lt;Optional&gt;</em> The minimum similarity between a tweet and a cluster to add the tweet to the cluster, defaults to 0.5.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max-intra-similarity</span></code>    <em>&lt;Optional&gt;</em> The maximum intra-similarity of documents in a cluster to consider it as a candidate topic, defaults to 0.8.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-tools.summarize">
<span id="summarization"></span><h2>Summarization<a class="headerlink" href="#module-tools.summarize" title="Permalink to this headline">¶</a></h2>
<p>The summarization tool receives a timeline and creates a summary for each node.
This tool is meant to summarize the <code class="docutils literal notranslate"><span class="pre">consume</span></code> tool’s output retrospectively, after all tweets have been assigned to the correct cluster.
Moreover, the summarization tool makes it easier to experiment with different parameters on the fly.</p>
<p>To run the script, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./tools/summarize.py <span class="se">\</span>
    --file data/timeline.json <span class="se">\</span>
    --method MMR <span class="se">\</span>
    --output data/summaries.json <span class="se">\</span>
    --documents <span class="m">50</span> <span class="se">\</span>
    --length <span class="m">280</span> <span class="se">\</span>
    --clean <span class="se">\</span>
    --with-query
</pre></div>
</div>
<p>Accepted arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-f</span> <span class="pre">--file</span></code>                 <em>&lt;Required&gt;</em> The path to the file containing the timeline to summarize.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">--method</span></code>               <em>&lt;Required&gt;</em> The method to use to generate summaries; supported: <cite>DGS</cite>, <cite>MMR</cite>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">--output</span></code>               <em>&lt;Required&gt;</em> The path to the file where to store the generated summaries.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-v</span> <span class="pre">--verbose</span></code>              <em>&lt;Optional&gt;</em> Print the summaries as they are generated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--documents</span></code>               <em>&lt;Optional&gt;</em> The maximum number of documents to use when summarizing, with a preference for quality documents, scored by the <code class="xref py py-class docutils literal notranslate"><span class="pre">TweetScorer</span></code>; defaults to all documents.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--length</span></code>                  <em>&lt;Optional&gt;</em> The length of each generated summary (in terms of the number of characters); defaults to 140 characters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--clean</span></code>                   <em>&lt;Optional&gt;</em> Clean the documents before summarizing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--lambda</span></code>                  <em>&lt;Optional&gt;</em> The lambda parameter to balance between relevance and non-redundancy (used only with the <a class="reference internal" href="summarization.html#summarization.algorithms.mmr.MMR" title="summarization.algorithms.mmr.MMR"><code class="xref py py-class docutils literal notranslate"><span class="pre">MMR</span></code></a> algorithm; defaults to 0.5).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--with-query</span></code>              <em>&lt;Optional&gt;</em> Use the centroid of each timeline node’s topics as a query for summarization (used only with the <a class="reference internal" href="summarization.html#summarization.algorithms.dgs.DGS" title="summarization.algorithms.dgs.DGS"><code class="xref py py-class docutils literal notranslate"><span class="pre">DGS</span></code></a> algorithm).</p></li>
</ul>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="vsm.html" class="btn btn-neutral float-right" title="2. Vector Space Model (VSM)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="config.html" class="btn btn-neutral float-left" title="0. Configuration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Nicholas Mamo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>