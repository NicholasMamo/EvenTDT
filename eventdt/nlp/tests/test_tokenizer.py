"""
Run unit tests on the :class:`~nlp.tokenizer.Tokenizer` class.
"""

from nltk.corpus import stopwords

import os
import sys
import time
import unittest

import warnings

path = os.path.join(os.path.dirname(__file__), "..")
if path not in sys.path:
	sys.path.append(path)

from tokenizer import Tokenizer

class TestTokenizer(unittest.TestCase):
	"""
	Run unit tests on the :class:`~nlp.tokenizer.Tokenizer` class.
	"""

	def ignore_warnings(test):
		"""
		A decorator function used to ignore NLTK warnings.
		From: http://www.neuraldump.net/2017/06/how-to-suppress-python-unittest-warnings/
		More about decorator functions: https://wiki.python.org/moin/PythonDecorators
		"""
		def perform_test(self, *args, **kwargs):
			with warnings.catch_warnings():
				warnings.simplefilter("ignore")
				test(self, *args, **kwargs)
		return perform_test

	def test_remove_mentions(self):
		"""
		Test the mention removal functionality.
		"""

		s = "@WRenard with the header"
		t = Tokenizer(remove_mentions=True)
		self.assertEqual([ "with", "the", "header" ], t.tokenize(s))

	def test_remove_multiple_mentions(self):
		"""
		Test that the mention removal functionality removes multiple mentions.
		"""

		s = "@WRenard with the header from @AdaStolsmo's cross"
		t = Tokenizer(remove_mentions=True)
		self.assertEqual([ "with", "the", "header", "from", "cross" ], t.tokenize(s))

	def test_keep_mentions(self):
		"""
		Test that mentions can be retained by the mention removal functionality.
		"""

		s = "@WRenard with the header"
		t = Tokenizer(remove_mentions=False)
		self.assertEqual([ "wrenard", "with", "the", "header" ], t.tokenize(s))

	def test_keep_multiple_mentions(self):
		"""
		Test that multiple mentions can be retained by the mention removal functionality.
		"""

		s = "@WRenard with the header from @AdaStolsmo's cross"
		t = Tokenizer(remove_mentions=False)
		self.assertEqual([ "wrenard", "with", "the", "header", "from", "adastolsmo", "cross" ], t.tokenize(s))

	def test_remove_hashtag(self):
		"""
		Test that the hashtag removal functionality removes a single hashtag.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL"
		t = Tokenizer(remove_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet" ], t.tokenize(s))

	def test_remove_hashtags(self):
		"""
		Test that the hashtag removal functionality removes all hashtags.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL #LEICHE"
		t = Tokenizer(remove_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet" ], t.tokenize(s))

	def test_remove_hashtags_mixed_case(self):
		"""
		Test that the hashtag removal functionality removes all hashtags, regardless of the case.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL #LeiChe"
		t = Tokenizer(remove_hashtags=True, split_hashtags=False, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet" ], t.tokenize(s))

	def test_retain_hashtags(self):
		"""
		Test that the hashtag removal functionality can retain all hashtags.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL #LEICHE"
		t = Tokenizer(remove_hashtags=False, split_hashtags=False, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'fpl', 'leiche' ], t.tokenize(s))

	def test_retain_hashtags_mixed_case(self):
		"""
		Test that the hashtag removal functionality can retain all hashtags, regardless of the case.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL #LeiChe"
		t = Tokenizer(remove_hashtags=False, split_hashtags=False, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'fpl', 'leiche' ], t.tokenize(s))

	def test_retain_hashtags_with_splitting(self):
		"""
		Test that when hashtags are removed, split hashtags are retained.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL #LeiChe"
		t = Tokenizer(remove_hashtags=True, split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'lei', 'che' ], t.tokenize(s))

	def test_split_hashtag(self):
		"""
		Test the hashtag splitting functionality.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #LeiChe"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'lei', 'che' ], t.tokenize(s))

	def test_split_hashtag_all_upper(self):
		"""
		Test that trying to split a hashtag that is made up of only uppercase letters does not split it.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #FPL"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'fpl' ], t.tokenize(s))

	def test_split_hashtag_all_lower(self):
		"""
		Test that trying to split a hashtag that is made up of only lowercase letters does not split it.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #fpl"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'fpl' ], t.tokenize(s))

	def test_split_hashtag_multiple_components(self):
		"""
		Test that hashtags with multiple components are split properly.
		"""

		s = "Hello! I'm Harry Styles, I'm sixteen and I work in a bakery #HappyBirthdayHarry"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "hello", "harry", "styles", "sixteen", "and", "work", "bakery", 'happy', 'birthday', 'harry' ], t.tokenize(s))

	def test_split_hashtag_repeated(self):
		"""
		Test that when a hashtag is repeated, splitting is applied to both.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #LeiChe #LeiChe"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'lei', 'che', 'lei', 'che' ], t.tokenize(s))

	def test_split_hashtag_with_numbers(self):
		"""
		Test that hashtags are treated as words when splitting hashtags.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #EPL2020"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'epl', '2020' ], t.tokenize(s))

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #2020EPL"
		t = Tokenizer(split_hashtags=True, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", '2020', 'epl' ], t.tokenize(s))

	def test_do_not_split_hashtags(self):
		"""
		Test that hashtags aren't split if the flag is not provided.
		"""

		s = "The Vardy party has gone very quiet ğŸ’¤ ğŸ˜¢ #EPL2020"
		t = Tokenizer(split_hashtags=False, stem=False)
		self.assertEqual([ "the", "vardy", "party", "has", "gone", "very", "quiet", 'epl2020' ], t.tokenize(s))

	def test_remove_numbers(self):
		"""
		Test that numbers are removed when the flag is provided.
		"""

		s = "There are 7 changes to last week's XI"
		t = Tokenizer(remove_numbers=True, stem=False)
		self.assertEqual([ "there", "are", "changes", "last", "week" ], t.tokenize(s))

	def test_remove_mixed_numbers(self):
		"""
		Test that numbers that are part of a string are not removed.
		"""

		s = "Nate Diaz wants Conor McGregor, Joe Rogan to apologize to Stephen A. Smith before UFC246"
		t = Tokenizer(remove_numbers=True, stem=False)
		self.assertEqual([ "nate", "diaz", "wants", "conor", "mcgregor", "joe", "rogan", "apologize", "stephen", "smith", "before", "ufc246" ], t.tokenize(s))

	def test_retain_years(self):
		"""
		Test that removing numbers does not remove years.
		"""

		s = "Real paid 5Mâ‚¬ for him back in 2016."
		t = Tokenizer(remove_numbers=True, stem=False)
		self.assertEqual([ "real", "paid", '5mâ‚¬', "for", "him", "back", "2016" ], t.tokenize(s))

		s = "Real paid 500 for him back in 2016."
		t = Tokenizer(remove_numbers=True, stem=False)
		self.assertEqual([ "real", "paid", "for", "him", "back", "2016" ], t.tokenize(s))

		s = "Real paid 50000 for him back in 2016."
		t = Tokenizer(remove_numbers=True, stem=False)
		self.assertEqual([ "real", "paid", "for", "him", "back", "2016" ], t.tokenize(s))

	def test_retain_numbers(self):
		"""
		Test that the tokenizer correctly retains numbers.
		"""

		s = "Real paid 5000000 for him back in 2016."
		t = Tokenizer(remove_numbers=False, stem=False)
		self.assertEqual([ "real", "paid", "5000000", "for", "him", "back", "2016" ], t.tokenize(s))

	def test_remove_url(self):
		"""
		Test the URL removal functionality.
		"""

		s = "Thank you @BillGates. It's amazing, almost as incredible as the fact that you use Gmail. https://t.co/drawyFHHQM"
		t = Tokenizer(remove_urls=True, stem=False)
		self.assertEqual([ "thank", "you", "amazing", "almost", "incredible", "the", "fact", "that", "you", "use", "gmail" ], t.tokenize(s))

	def test_remove_url_without_protocol(self):
		"""
		Test the URL removal functionality when there is no protocol.
		"""

		s = "Thank you @BillGates. It's amazing, almost as incredible as the fact that you use Gmail. t.co/drawyFHHQM"
		t = Tokenizer(remove_urls=True, stem=False)
		self.assertEqual([ "thank", "you", "amazing", "almost", "incredible", "the", "fact", "that", "you", "use", "gmail" ], t.tokenize(s))

	def test_remove_url_with_http_protocol(self):
		"""
		Test the URL removal functionality when the protocol is http.
		"""

		s = "Thank you @BillGates. It's amazing, almost as incredible as the fact that you use Gmail. http://t.co/drawyFHHQM"
		t = Tokenizer(remove_urls=True, stem=False)
		self.assertEqual([ "thank", "you", "amazing", "almost", "incredible", "the", "fact", "that", "you", "use", "gmail" ], t.tokenize(s))

	def test_remove_subdomain(self):
		"""
		Test that URL removal includes subdomains.
		"""

		s = "Visit Multiplex's documentation for more information: https://nicholasmamo.github.io/multiplex-plot/"
		t = Tokenizer(remove_urls=True, stem=False)
		self.assertEqual([ "visit", "multiplex", "documentation", "for", "more", "information" ], t.tokenize(s))

	def test_remove_subdomain_without_protocol(self):
		"""
		Test that URL removal includes subdomains even if they have no protocol.
		"""

		s = "Visit Multiplex's documentation for more information: nicholasmamo.github.io/multiplex-plot/"
		t = Tokenizer(remove_urls=True, stem=False)
		self.assertEqual([ "visit", "multiplex", "documentation", "for", "more", "information" ], t.tokenize(s))

	def test_retain_url(self):
		"""
		Test the URL retention functionality.
		"""

		s = "Thank you @BillGates. It's amazing, almost as incredible as the fact that you use Gmail. https://t.co/drawyFHHQM"
		t = Tokenizer(remove_urls=False, stem=False)
		self.assertEqual([ "thank", "you", "amazing", "almost", "incredible", "the", "fact", "that", "you", "use", "gmail", "https", "drawyfhhqm" ], t.tokenize(s))

	def test_alt_code_removal(self):
		"""
		Test the alt-code removal functionality works as it is supposed to.
		"""

		s = "Our prediction based on #FIFA Rankings, &amp; Country Risk Ratings"
		t = Tokenizer(remove_alt_codes=True)
		self.assertEqual([ "our", "predict", "base", "fifa", "rank", "countri", "risk", "rate" ], t.tokenize(s))

	def test_retain_alt_codes(self):
		"""
		Test that when alt-codes are not set to be removed, they are retained.
		"""

		s = "Our prediction based on #FIFA Rankings, &amp; Country Risk Ratings"
		t = Tokenizer(remove_alt_codes=False)
		self.assertEqual([ "our", "predict", "base", "fifa", "rank", "amp", "countri", "risk", "rate" ], t.tokenize(s))

	def test_word_normalization(self):
		"""
		Test that word normalization reduces normalized characters.
		"""

		s = "YYYYYEEESSSSSSSSS OOXXXXXXXXXXXX!!!"
		t = Tokenizer(normalize_words=True, character_normalization_count=2, stem=False, min_length=2)
		self.assertEqual([ "yes", "ox" ], t.tokenize(s))

	def test_word_normalization_character_limit(self):
		"""
		Test that word normalization reduces normalized characters.
		"""

		s = "YYYYYEEESSSSSSSSS OOXXXXXXXXXXXX!!!"
		t = Tokenizer(normalize_words=True, character_normalization_count=2, stem=False, min_length=2)
		self.assertEqual([ "yes", "ox" ], t.tokenize(s))

	def test_word_normalization_character_exact_limit(self):
		"""
		Test that word normalization reduces repeated characters if the count is exact.
		"""

		s = "YYYYYEEESSSSSSSSS OOXXXXXXXXXXXX!!!"
		t = Tokenizer(normalize_words=True, character_normalization_count=3, stem=False, min_length=2)
		self.assertEqual([ "yes", "oox" ], t.tokenize(s))

	def test_word_normalization_character_high_limit(self):
		"""
		Test that word normalization does not reduce repeated characters if the limit is higher.
		"""

		s = "YYYYYEEESSSSSSSSS OOXXXXXXXXXXXX!!!"
		t = Tokenizer(normalize_words=True, character_normalization_count=4, stem=False, min_length=2)
		self.assertEqual([ "yeees", "oox" ], t.tokenize(s))

	def test_no_word_normalization(self):
		"""
		Test that word normalization is not permitted, no words are normalized.
		"""

		s = "YYYYYEEESSSSSSSSS OOXXXXXXXXXXXX!!!"
		t = Tokenizer(normalize_words=False, character_normalization_count=2, stem=False, min_length=2)
		self.assertEqual([ "yyyyyeeesssssssss", "ooxxxxxxxxxxxx" ], t.tokenize(s))

	def test_unicode_removal(self):
		"""
		Test that the unicode entity removal functionality removes unicode characters.
		"""

		s = "\u0632\u0648\u062f_\u0641\u0648\u0644\u0648\u0631\u0632_\u0645\u0639_\u0627\u0644\u0645\u0628\u0627\u062d\u062b"
		t = Tokenizer(remove_unicode_entities=True, stem=False, min_length=1, remove_punctuation=False)
		self.assertEqual([ '___' ], t.tokenize(s))

	def test_unicode_removal_includes_emojis(self):
		"""
		Test that the unicode entity removal functionality also removes emojis.
		"""

		s = "Je veux ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¦"
		t = Tokenizer(remove_unicode_entities=True, stem=False, min_length=1, remove_punctuation=False)
		self.assertEqual([ 'je', 'veux' ], t.tokenize(s))

	def test_retain_unicode(self):
		"""
		Test unicode character retention.
		"""

		s = "\u0632\u0648\u062f_\u0641\u0648\u0644\u0648\u0631\u0632_\u0645\u0639_\u0627\u0644\u0645\u0628\u0627\u062d\u062b"
		t = Tokenizer(remove_unicode_entities=False, stem=False, min_length=1, remove_punctuation=False)
		self.assertEqual([ 'Ø²ÙˆØ¯_ÙÙˆÙ„ÙˆØ±Ø²_Ù…Ø¹_Ø§Ù„Ù…Ø¨Ø§Ø­Ø«' ], t.tokenize(s))

	def test_unicode_retention_includes_emojis(self):
		"""
		Test that the unicode entity retention also allows emojis.
		"""

		s = "Je veux ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¦"
		t = Tokenizer(remove_unicode_entities=False, stem=False, min_length=1, remove_punctuation=False)
		self.assertEqual([ 'je', 'veux', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¦' ], t.tokenize(s))

	def test_minimum_length_minimum(self):
		"""
		Test that when the minimum length is 1, all non-empty tokens are retained.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(min_length=1, stem=False)
		self.assertEqual([ 'gelson', 'martins', 'tries', 'to', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_minimum_length_exact(self):
		"""
		Test that the minimum length does not filter tokens with exact length.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(min_length=2, stem=False)
		self.assertEqual([ 'gelson', 'martins', 'tries', 'to', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_minimum_length_filter(self):
		"""
		Test that the minimum length filters tokens with length less than it.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(min_length=3, stem=False)
		self.assertEqual([ 'gelson', 'martins', 'tries', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_no_stopwords(self):
		"""
		Test that when no stopwords are provided, no tokens are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False)
		self.assertEqual([ 'gelson', 'martins', 'tries', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_stopwords_empty_list(self):
		"""
		Test that when an empty list of stopwords is provided, no tokens are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False, stopwords=[])
		self.assertEqual([ 'gelson', 'martins', 'tries', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_stopwords_empty_dict(self):
		"""
		Test that when an empty dictionary of stopwords is provided, no tokens are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False, stopwords={})
		self.assertEqual([ 'gelson', 'martins', 'tries', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_stopwords_list(self):
		"""
		Test that when a list of stopwords is provided, tokens in that list are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False, stopwords=[ 'tries' ])
		self.assertEqual([ 'gelson', 'martins', 'shove', 'the', 'referee' ], t.tokenize(s))

	def test_stopwords_dict(self):
		"""
		Test that when a dictionary of stopwords is provided, tokens in that dictionary are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False, stopwords={ 'tries': 1 })
		self.assertEqual([ 'gelson', 'martins', 'shove', 'the', 'referee' ], t.tokenize(s))

	@ignore_warnings
	def test_stopwords_nltk(self):
		"""
		Test that when the list of NLTK stopwords is provided, the terms are removed.
		"""

		s = "Gelson Martins tries to shove the referee."
		t = Tokenizer(stem=False, stopwords=list(stopwords.words("english")))
		self.assertEqual([ 'gelson', 'martins', 'tries', 'shove', 'referee' ], t.tokenize(s))

	def test_case_folding(self):
		"""
		Test that when case folding is set, all tokens are set to lower-case.
		"""

		s = "#BREAKING Nine illegal miners killed by rival workers in S. Africa say police"
		t = Tokenizer(case_fold=True, stem=False)
		self.assertEqual([ "breaking", "nine", "illegal", 'miners', 'killed', 'rival', 'workers', 'africa', 'say', 'police' ], t.tokenize(s))

	def test_no_case_folding(self):
		"""
		Test that when no case folding is set, all tokens are returned as provided.
		"""

		s = "#BREAKING Nine illegal miners killed by rival workers in S. Africa say police"
		t = Tokenizer(case_fold=False, stem=False)
		self.assertEqual([ "BREAKING", "Nine", "illegal", 'miners', 'killed', 'rival', 'workers', 'Africa', 'say', 'police' ], t.tokenize(s))

	def test_punctuation_removal(self):
		"""
		Test that when there is punctuation removal, no punctuation is retained.
		"""

		s = "Toko-Ekambi scores, assisted by Mendes!"
		t = Tokenizer(remove_punctuation=True, stem=False)
		self.assertEqual([ "toko", "ekambi", "scores", "assisted", "mendes" ], t.tokenize(s))

	def test_no_punctuation_removal(self):
		"""
		Test that when there is no punctuation removal, the punctuation sticks to the adjacent token.
		"""

		s = "Toko-Ekambi scores, assisted by Mendes!"
		t = Tokenizer(remove_punctuation=False, stem=False)
		self.assertEqual([ "toko-ekambi", "scores,", "assisted", "mendes!" ], t.tokenize(s))

	def test_punctuation_retains_emojis(self):
		"""
		Test that punctuation removal does not remove emojis.
		"""

		s = "Je veux ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¦"
		t = Tokenizer(remove_unicode_entities=False, stem=False, min_length=1, remove_punctuation=True)
		self.assertEqual([ 'je', 'veux', 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¦' ], t.tokenize(s))

	def test_punctuation_retains_unicode(self):
		"""
		Test that punctuation removal does not remove unicode characters.
		"""

		s = "\u0632\u0648\u062f_\u0641\u0648\u0644\u0648\u0631\u0632_\u0645\u0639_\u0627\u0644\u0645\u0628\u0627\u062d\u062b"
		t = Tokenizer(remove_unicode_entities=False, stem=False, min_length=1, remove_punctuation=True)
		self.assertEqual(['Ø²ÙˆØ¯', 'ÙÙˆÙ„ÙˆØ±Ø²', 'Ù…Ø¹', 'Ø§Ù„Ù…Ø¨Ø§Ø­Ø«'], t.tokenize(s))

	def test_stemming(self):
		"""
		Test that when stemming is performed, suffixes are removed.
		"""

		s = "Toko-Ekambi scores, assisted by Mendes!"
		t = Tokenizer(stem=True)
		self.assertEqual([ "toko", "ekambi", "score", "assist", "mend" ], t.tokenize(s))

	def test_stemming_cache(self):
		"""
		Test that the stemmer uses a cache.
		"""

		s = "Toko-Ekambi scores, assisted by Mendes!"
		t = Tokenizer(stem=True)
		start = time.time()
		self.assertEqual([ "toko", "ekambi", "score", "assist", "mend" ], t.tokenize(s))
		elapsed = time.time() - start
		start = time.time()
		self.assertEqual({ 'toko', 'ekambi', 'scores', 'assisted', 'mendes' }, set(t.stem_cache.keys()))
		self.assertEqual('toko', t.stem_cache.get('toko'))
		self.assertEqual('ekambi', t.stem_cache.get('ekambi'))
		self.assertEqual('score', t.stem_cache.get('scores'))
		self.assertEqual('assist', t.stem_cache.get('assisted'))
		self.assertEqual('mend', t.stem_cache.get('mendes'))
		t.tokenize(s)
		self.assertLess(time.time() - start, elapsed)

	def test_no_stemming(self):
		"""
		Test that when there is no stemming, suffixes are retained.
		"""

		s = "Toko-Ekambi scores, assisted by Mendes!"
		t = Tokenizer(stem=False)
		self.assertEqual([ "toko", "ekambi", "scores", "assisted", "mendes" ], t.tokenize(s))

	def test_retain_special_characters(self):
		"""
		Test that when special_characters are not normalized, they are retained.
		"""

		s = 'On ne lÃ¢che rien'
		t = Tokenizer(stem=False, min_length=2, normalize_special_characters=False)
		self.assertEqual([ 'on', 'ne', 'lÃ¢che', 'rien' ], t.tokenize(s))

	def test_retain_french_special_characters(self):
		"""
		Test that when French special_characters are not normalized, they are retained.
		"""

		s = 'On ne lÃ¢che rien'
		t = Tokenizer(stem=False, min_length=2, normalize_special_characters=False)
		self.assertEqual([ 'on', 'ne', 'lÃ¢che', 'rien' ], t.tokenize(s))

	def test_normalize_french_special_characters(self):
		"""
		Test that when French special_characters are normalized, they are converted to the correct character.
		"""

		s = 'Il joue trÃ¨s gros... Peut-Ãªtre sa derniÃ¨re chance'
		t = Tokenizer(stem=False, normalize_special_characters=True)
		self.assertEqual([ 'joue', 'tres', 'gros', 'peut', 'etre', 'derniere', 'chance' ], t.tokenize(s))

	def test_retain_germanic_special_characters(self):
		"""
		Test that when Germanic special_characters are not normalized, they are retained.
		"""

		s = 'SÃ¥ leker Ã–zil Ã¤ntligen'
		t = Tokenizer(stem=False, min_length=2, normalize_special_characters=False)
		self.assertEqual([ 'sÃ¥', 'leker', 'Ã¶zil', 'Ã¤ntligen' ], t.tokenize(s))

	def test_normalize_germanic_special_characters(self):
		"""
		Test that when Germanic special_characters are normalized, they are converted to the correct character.
		"""

		s = 'SÃ¥ leker Ã–zil Ã¤ntligen'
		t = Tokenizer(stem=False, min_length=2, normalize_special_characters=True)
		self.assertEqual([ 'sa', 'leker', 'ozil', 'antligen' ], t.tokenize(s))

	def test_retain_maltese_special_characters(self):
		"""
		Test that when Maltese special_characters are not normalized, they are retained.
		"""

		s = 'ÄŠikku Å¼ar lil Ä anni il-Ä¦amrun'
		t = Tokenizer(stem=False, normalize_special_characters=False)
		self.assertEqual([ 'Ä‹ikku', 'Å¼ar', 'lil', 'Ä¡anni', 'Ä§amrun' ], t.tokenize(s))

	def test_normalize_maltese_special_characters(self):
		"""
		Test that when Maltese special_characters are normalized, they are converted to the correct character.
		Note that this does not apply to Ä¦/Ä§.
		"""

		s = 'ÄŠikku Å¼ar lil Ä anni il-Ä¦amrun'
		t = Tokenizer(stem=False, normalize_special_characters=True)
		self.assertEqual([ 'cikku', 'zar', 'lil', 'ganni', 'Ä§amrun' ], t.tokenize(s))
