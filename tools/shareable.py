#!/usr/bin/env python3

"""
The shareable tool is used to convert tweet datasets into shareable datasets.
As per `Twitter's Developer Policy <https://developer.twitter.com/en/developer-terms/policy>`_, only tweet IDs can be shared.
Therefore this tool retains only the tweet IDs from corpora.

To run this tool, use the following:

.. code-block:: bash

    ./tools/shareable.py \\
    --file data/corpus.json \\
    --output data/shareable.json

By default, the tool saves the meta details to a file in the same directory.
However, you can save the meta file in a different path by providing the ``--meta`` parameter:

.. code-block:: bash

    ./tools/shareable.py \\
    --file data/corpus.json \\
    --output data/shareable.json \\
    --meta meta/shareable.json

The full list of accepted arguments:

    - ``-f --file``             *<Required>* The original corpus collected by the :mod:`~tools.collect` tool, or an archive of files, or a timeline generated by the :mod:`~tools.consume` tool.
    - ``-o --output``           *<Required>* The file or directory where to save the shareable corpus.
    - ``--meta``                *<Optional>* The file where to save the meta data, defaults to [--file].meta.

The tool writes one tweet ID on each line in the output file.
"""

import argparse
import json
import os
import sys
import tarfile

file_path = os.path.dirname(os.path.abspath(__file__))
root = os.path.join(file_path, '..')
lib = os.path.join(root, 'eventdt')
sys.path.insert(-1, root)
sys.path.insert(-1, lib)

from logger import logger
from objects import Exportable
import tools
import twitter

def setup_args():
    """
    Set up and get the list of command-line arguments.

    Accepted arguments:

        - ``-f --file``             *<Required>* The original corpus collected by the :mod:`~tools.collect` tool, or an archive of files, or a timeline generated by the :mod:`~tools.consume` tool.
        - ``-o --output``           *<Required>* The file or directory where to save the shareable corpus.
        - ``--meta``                *<Optional>* The file where to save the meta data, defaults to [--file].meta.

    :return: The command-line arguments.
    :rtype: :class:`argparse.Namespace`
    """

    parser = argparse.ArgumentParser(description="Make a dataset shareable.")

    parser.add_argument('-f', '--file', type=str, required=True,
                        help='<Required> The original corpus collected by the `collect` tool, or an archive of files, or a timeline generated by the `consume` tool.')
    parser.add_argument('-o', '--output', type=str, required=True,
                        help='<Required> The file or directory where to save the shareable corpus.')
    parser.add_argument('--meta', type=str, required=False,
                        help='<Optional> The file where to save the meta data, defaults to [--file].meta.')

    args = parser.parse_args()
    return args

def main():
    """
    Main program loop.
    """

    # set up the arguments and prepare the data directory.
    args = setup_args()
    cmd = tools.meta(args)
    pcmd = tools.meta(args)
    tools.save(args.output, { }) # to create the directory if it doesn't exist

    if tarfile.is_tarfile(args.file):
        write_archive(args.file, args.output)
    elif is_timeline(args.file):
        write_timeline(args.file, args.output)
    else:
        write(args.file, args.output)

    meta = args.meta or tools.meta_file(args.output)
    pcmd['meta'] = meta
    tools.save(meta, { 'cmd': cmd, 'pcmd': pcmd })

def is_timeline(file):
    """
    Check whether the given file contains a timeline generated by the :mod:`~tools.consume` tool.

    :param file: The path to the original corpus of tweets.
    :type file: str

    :return: A boolean indicating whether the given file contains a timeline.
    :rtype bool.
    """

    with open(file, 'r') as infile:
        line = infile.readline()
        data = json.loads(line)
        return 'timeline' in data

def write_archive(file, output):
    """
    Make the given archive shareable, writing the tweet IDs of each file separately into the given output directory.

    :param file: The path to the original archive, containing corpora of tweets.
    :type file: str
    :param output: The path where to write the tweet IDs.
    :type output: str
    """

    output = os.path.split(output)[0] # retain only the directory from the output as each file be anonymized separately
    with tarfile.open(file) as archive:
        for member in archive:
            path, filename = os.path.split(member.name)
            if filename in [ 'meta.json', 'understanding.json', 'event.json', 'sample.json' ]:
                # create the folder structure if need be
                outputdir = os.path.join(output, path)
                if not os.path.exists(outputdir):
                    os.makedirs(outputdir)

            if filename in [ 'understanding.json', 'event.json', 'sample.json' ]:
                # write the file's tweet IDs to a new file
                with open(os.path.join(outputdir, filename), 'w') as outfile, \
                     archive.extractfile(member) as _file:
                    try: 
                        for line in _file:
                            tweet = json.loads(line)
                            outfile.write(f"{ twitter.id(tweet) }\n")
                    except Exception as e:
                        logger.error(f"Skipping {file}/{path}/{filename}")
            elif filename in [ 'meta.json' ]:
                # write the file's tweet IDs to a new file
                with open(os.path.join(outputdir, filename), 'w') as outfile, \
                     archive.extractfile(member) as _file:
                    for line in _file:
                        outfile.write(f"{ json.dumps(json.loads(line)) }\n")
            else:
                logger.warning(f"Unknown file {filename}")

def write_timeline(file, output):
    """
    Make the given timeline shareable, replacing all tweets with IDs instead.

    :param file: The path to the original timeline.
    :type file: str
    :param output: The path where to write the new, anonymized timeline.
    :type output: str
    """

    with open(file, 'r') as infile, \
         open(output, 'w') as outfile:
        line = infile.readline()
        data = Exportable.decode(json.loads(line))
        timelines = data['timeline'] if type(data['timeline']) is list else [ data['timeline'] ]

        # go through each document in each node in every timeline and anonymize it
        for timeline in timelines:
            for node in timeline.nodes:
                for document in node.get_all_documents():
                    if document.tweet:
                        document.attributes['tweet'] = { 'id': twitter.id(document.tweet) }

        # keep the original structure of the timeline, whether one timeline or a list
        data['timeline'] = timelines if type(data['timeline']) is list else timelines[0]
        outfile.write(json.dumps(Exportable.encode(data)))

def write(file, output):
    """
    Make the given file shareable, writing the tweet IDs into the given output file.

    :param file: The path to the original corpus of tweets.
    :type file: str
    :param output: The path where to write the tweet IDs.
    :type output: str
    """

    with open(file, 'r') as infile, \
         open(output, 'w') as outfile:
        for line in infile:
            tweet = json.loads(line)
            outfile.write(f"{ twitter.id(tweet) }\n")

if __name__ == "__main__":
    main()
